{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9757873,"sourceType":"datasetVersion","datasetId":5974990}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate==0.26.0\n!pip install diffusers==0.16.0\n!pip install jax==0.4.23 jaxlib==0.4.23","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:39:28.178037Z","iopub.execute_input":"2024-11-05T19:39:28.178540Z","iopub.status.idle":"2024-11-05T19:40:12.794013Z","shell.execute_reply.started":"2024-11-05T19:39:28.178499Z","shell.execute_reply":"2024-11-05T19:40:12.792962Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting accelerate==0.26.0\n  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (2.4.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (0.25.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.0) (0.4.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.26.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.26.0) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\nDownloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-0.26.0\nCollecting diffusers==0.16.0\n  Downloading diffusers-0.16.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (0.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (7.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.16.0) (2.32.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.16.0) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.16.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.16.0) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.16.0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.16.0) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.16.0) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.16.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.16.0) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.13.2->diffusers==0.16.0) (3.1.2)\nDownloading diffusers-0.16.0-py3-none-any.whl (934 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.8/934.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.16.0\nCollecting jax==0.4.23\n  Downloading jax-0.4.23-py3-none-any.whl.metadata (24 kB)\nCollecting jaxlib==0.4.23\n  Downloading jaxlib-0.4.23-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23) (0.3.2)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax==0.4.23) (1.14.1)\nDownloading jax-0.4.23-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading jaxlib-0.4.23-cp310-cp310-manylinux2014_x86_64.whl (77.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: jaxlib, jax\n  Attempting uninstall: jaxlib\n    Found existing installation: jaxlib 0.4.26.dev20240620\n    Uninstalling jaxlib-0.4.26.dev20240620:\n      Successfully uninstalled jaxlib-0.4.26.dev20240620\n  Attempting uninstall: jax\n    Found existing installation: jax 0.4.26\n    Uninstalling jax-0.4.26:\n      Successfully uninstalled jax-0.4.26\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\norbax-checkpoint 0.6.4 requires jax>=0.4.26, but you have jax 0.4.23 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed jax-0.4.23 jaxlib-0.4.23\n","output_type":"stream"}]},{"cell_type":"code","source":"# Clone the ImageReward repository (containing data for testing)\n!git clone https://github.com/THUDM/ImageReward.git\n!cd ImageReward\n\n# Install the integrated package `image-reward`\n!pip install image-reward\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:40:12.795869Z","iopub.execute_input":"2024-11-05T19:40:12.796183Z","iopub.status.idle":"2024-11-05T19:41:09.716018Z","shell.execute_reply.started":"2024-11-05T19:40:12.796149Z","shell.execute_reply":"2024-11-05T19:41:09.715023Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'ImageReward'...\nremote: Enumerating objects: 224, done.\u001b[K\nremote: Counting objects: 100% (91/91), done.\u001b[K\nremote: Compressing objects: 100% (48/48), done.\u001b[K\nremote: Total 224 (delta 52), reused 48 (delta 42), pack-reused 133 (from 1)\u001b[K\nReceiving objects: 100% (224/224), 4.30 MiB | 34.92 MiB/s, done.\nResolving deltas: 100% (89/89), done.\nCollecting image-reward\n  Downloading image_reward-1.5-py3-none-any.whl.metadata (12 kB)\nCollecting timm==0.6.13 (from image-reward)\n  Downloading timm-0.6.13-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: transformers>=4.27.4 in /opt/conda/lib/python3.10/site-packages (from image-reward) (4.45.1)\nCollecting fairscale==0.4.13 (from image-reward)\n  Downloading fairscale-0.4.13.tar.gz (266 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.13.4 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.25.1)\nRequirement already satisfied: diffusers>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.16.0)\nRequirement already satisfied: accelerate>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.26.0)\nRequirement already satisfied: datasets>=2.11.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (3.0.1)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from fairscale==0.4.13->image-reward) (2.4.0)\nRequirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from fairscale==0.4.13->image-reward) (1.26.4)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.6.13->image-reward) (0.19.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.6.13->image-reward) (6.0.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (5.9.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.11.0->image-reward) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.9.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (10.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (7.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (2024.5.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.4->image-reward) (4.12.2)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.27.4->image-reward) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.16.0->image-reward) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (3.1.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers>=0.16.0->image-reward) (3.19.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.11.0->image-reward) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->fairscale==0.4.13->image-reward) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->fairscale==0.4.13->image-reward) (1.3.0)\nDownloading image_reward-1.5-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m28.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading timm-0.6.13-py3-none-any.whl (549 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fairscale\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332106 sha256=4a536469f1837566a02762321f90896b819489f5fe1d6d85b60fc4aecd2ba81e\n  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\nSuccessfully built fairscale\nInstalling collected packages: fairscale, timm, image-reward\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\nSuccessfully installed fairscale-0.4.13 image-reward-1.5 timm-0.6.13\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-st8fwvhq\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-st8fwvhq\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=e8b0f2d512f6de683121cd6d10f2506f2216f0b30362e68bfb54a59943966d59\n  Stored in directory: /tmp/pip-ephem-wheel-cache-lamd_szr/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport copy\nfrom dataclasses import dataclass\nimport functools\nimport json\nimport logging\nimport os\nimport pickle\nimport random\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed \n\nimport datasets\nimport dataclasses\nfrom datasets import load_dataset\nimport diffusers\nfrom diffusers import UNet2DConditionModel\nfrom diffusers.loaders import AttnProcsLayers\nfrom diffusers.models.cross_attention import LoRACrossAttnProcessor\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils import deprecate\nimport logging\nimport ImageReward as imagereward\nfrom diffusers.utils.import_utils import is_xformers_available\n\nimport transformers\nfrom transformers import CLIPModel, CLIPProcessor  # pylint: disable=g-multiple-import\nfrom transformers import CLIPTextModel, CLIPTokenizer  # pylint: disable=g-multiple-import\n\nfrom typing import Optional, Tuple, Union\nfrom diffusers import DDIMScheduler\nfrom diffusers.schedulers.scheduling_ddim import DDIMSchedulerOutput\nfrom diffusers.utils import randn_tensor\nimport torch\nfrom torch.distributions import Normal\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.parallel import DistributedDataParallel\nimport torch.utils.checkpoint\nfrom tqdm.auto import tqdm\n\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom diffusers import StableDiffusionPipeline\n\nimport datasets\nfrom datasets import load_dataset\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:09.717746Z","iopub.execute_input":"2024-11-05T19:41:09.718165Z","iopub.status.idle":"2024-11-05T19:41:30.089437Z","shell.execute_reply.started":"2024-11-05T19:41:09.718119Z","shell.execute_reply":"2024-11-05T19:41:30.088660Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n  deprecate(\n","output_type":"stream"}]},{"cell_type":"code","source":"@dataclass\nclass Args:\n    single_prompt: str = \"A green colored rabbit.\"\n    p_batch_size: int = 4\n    reward_weight: int = 10\n    kl_weight: float = 0.01\n    learning_rate: float = 1e-5\n    adam_beta1: float = 0.9\n    adam_beta2: float = 0.999\n    lr_warmup_steps: int = 0\n    single_flag: int = 1\n    prompt_path: str = \"/kaggle/input/drawbench/data_meta.json\"\n    gradient_accumulation_steps: int = 12\n    clip_norm: float = 0.1\n    g_batch_size: int = 10\n    multi_gpu: int = 1\n    max_train_steps: int = 1000\n    v_flag: int = 1\n    prompt_category: str = \"all\"\n    p_step: int = 5\n    lora_rank: int = 4\n    kl_warmup: int = -1\n    sft_initialization: int = 0\n    reward_flag: int = 0\n    reward_filter: int = 0\n    v_lr: float = 1e-4\n    v_batch_size: int = 24\n    v_step: int = 5\n    mixed_precision: str = \"fp16\"\n    report_to: str = \"tensorboard\"\n    output_dir: str = \"online_model\"\n    logging_dir: str = \"logs\"\n    seed: str = None\n    pretrained_model_name_or_path: str = \"runwayml/stable-diffusion-v1-5\"\n    revision: str = None\n    prompt_category: str = \"all\"\n    train_batch_size: int = 8\n    g_step: int = 1\n    gradient_checkpointing: bool = True\n    adam_weight_decay: float = 0.9\n    adam_epsilon: float = 1e-08\n    lr_scheduler: str = \"constant\"\n    num_train_epochs: int = 5\n    buffer_size: int = 500\n    num_samples: int = 1\n        \n@dataclasses.dataclass(frozen=False)\nclass TrainPolicyFuncData:\n    tot_p_loss: float = 0\n    tot_ratio: float = 0\n    tot_kl: float = 0\n    tot_grad_norm: float = 0\n        \nlogger = get_logger(__name__, log_level=\"INFO\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.091710Z","iopub.execute_input":"2024-11-05T19:41:30.092338Z","iopub.status.idle":"2024-11-05T19:41:30.106774Z","shell.execute_reply.started":"2024-11-05T19:41:30.092302Z","shell.execute_reply":"2024-11-05T19:41:30.105743Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DDIMSchedulerExtended(DDIMScheduler):\n    \"\"\"Extension of diffusers.DDIMScheduler.\"\"\"\n    \n    def _get_variance_logprob(self, timestep, prev_timestep):\n        alpha_prod_t = self.alphas_cumprod.to(timestep.device)[timestep]\n        mask_a = (prev_timestep >= 0).int().to(timestep.device)\n        mask_b = 1 - mask_a\n        alpha_prod_t_prev = (\n            self.alphas_cumprod.to(timestep.device)[prev_timestep] * mask_a\n            + self.final_alpha_cumprod.to(timestep.device) * mask_b\n        )\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n        variance = (beta_prod_t_prev / beta_prod_t) * (\n            1 - alpha_prod_t / alpha_prod_t_prev\n        )\n        return variance\n    \n    def step_logprob(\n      self,\n      model_output,\n      timestep,\n      sample,\n      eta = 1.0,\n      use_clipped_model_output = False,\n      generator=None,\n      variance_noise = None,\n      return_dict = True):  # pylint: disable=g-bare-generic\n        \"\"\"Predict the sample at the previous timestep by reversing the SDE.\n\n        Core function to propagate the diffusion process from the learned model\n        outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion\n                  model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`): current instance of sample being created\n              by diffusion process.\n            eta (`float`): weight of noise for added noise in diffusion step.\n            use_clipped_model_output (`bool`): if `True`, compute \"corrected\"\n              `model_output` from the clipped predicted original sample. Necessary\n              because predicted original sample is clipped to [-1, 1] when\n              `self.config.clip_sample` is `True`. If no clipping has happened,\n              \"corrected\" `model_output` would coincide with the one provided as\n              input and `use_clipped_model_output` will have not effect.\n            generator: random number generator.\n            variance_noise (`torch.FloatTensor`): instead of generating noise for\n                  the variance using `generator`, we can directly provide the noise for\n                  the variance itself. This is useful for methods such as\n                  CycleDiffusion. (https://arxiv.org/abs/2210.05559)\n            return_dict (`bool`): option for returning tuple rather than\n                  DDIMSchedulerOutput class\n\n        Returns:\n            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:\n            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is\n                True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n\n            log_prob (`torch.FloatTensor`): log probability of the sample.\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                  \"Number of inference steps is 'None', you need to run 'set_timesteps'\"\n                  \" after creating the scheduler\"\n            )\n\n        # pylint: disable=line-too-long\n        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n        # Ideally, read DDIM paper in-detail understanding\n\n        # Notation (<variable name> -> <name in paper>\n        # - pred_noise_t -> e_theta(x_t, t)\n        # - pred_original_sample -> f_theta(x_t, t) or x_0\n        # - std_dev_t -> sigma_t\n        # - eta -> η\n        # - pred_sample_direction -> \"direction pointing to x_t\"\n        # - pred_prev_sample -> \"x_t-1\"\n\n        # 1. get previous step value (=t-1)\n        prev_timestep = (\n            timestep - self.config.num_train_timesteps // self.num_inference_steps\n        )\n\n        # 2. compute alphas, betas\n        alpha_prod_t = self.alphas_cumprod.to(timestep.device)[timestep]\n        # alpha_prod_t = alpha_prod_t.to(torch.float16)\n        mask_a = (prev_timestep >= 0).int().to(timestep.device)\n        mask_b = 1 - mask_a\n        alpha_prod_t_prev = (\n            self.alphas_cumprod.to(timestep.device)[prev_timestep] * mask_a\n            + self.final_alpha_cumprod.to(timestep.device) * mask_b\n        )\n        beta_prod_t = 1 - alpha_prod_t\n\n        # 3. compute predicted original sample from predicted noise also called\n        # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        if self.config.prediction_type == \"epsilon\":\n            pred_original_sample = (\n                  sample - beta_prod_t ** (0.5) * model_output\n          ) / alpha_prod_t ** (0.5)\n        elif self.config.prediction_type == \"sample\":\n            pred_original_sample = model_output\n        elif self.config.prediction_type == \"v_prediction\":\n            pred_original_sample = (alpha_prod_t**0.5) * sample - (\n              beta_prod_t**0.5) * model_output\n            # predict V\n            model_output = (alpha_prod_t**0.5) * model_output + (\n              beta_prod_t**0.5) * sample\n        else:\n            raise ValueError(\n              f\"prediction_type given as {self.config.prediction_type} must be one\"\n              \" of `epsilon`, `sample`, or `v_prediction`\")\n\n        # 4. Clip \"predicted x_0\"\n        if self.config.clip_sample:\n            pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n\n        # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n        # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n        variance = self._get_variance_logprob(timestep, prev_timestep).to(\n            dtype=sample.dtype\n        )\n        std_dev_t = (eta * variance ** (0.5)).to(dtype=sample.dtype)\n\n        if use_clipped_model_output:\n            # the model_output is always re-derived from the clipped x_0 in Glide\n            model_output = (\n                sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n\n        # pylint: disable=line-too-long\n        # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output\n\n        # pylint: disable=line-too-long\n        # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        prev_sample = (\n            alpha_prod_t_prev ** (0.5) * pred_original_sample\n            + pred_sample_direction\n        )\n\n        if eta > 0:\n            device = model_output.device\n            if variance_noise is not None and generator is not None:\n                raise ValueError(\n                    \"Cannot pass both generator and variance_noise. Please make sure\"\n                    \" that either `generator` or `variance_noise` stays `None`.\"\n                )\n\n            if variance_noise is None:\n                variance_noise = randn_tensor(\n                    model_output.shape,\n                    generator=generator,\n                    device=device,\n                    dtype=model_output.dtype)\n            variance = std_dev_t * variance_noise\n            dist = Normal(prev_sample, std_dev_t)\n            prev_sample = prev_sample.detach().clone() + variance\n            log_prob = (\n                dist.log_prob(prev_sample.detach().clone())\n                .mean(dim=-1)\n                .mean(dim=-1)\n                .mean(dim=-1)\n                .detach()\n                .cpu())\n        if not return_dict:\n            return (prev_sample,)\n        \n        return (\n            DDIMSchedulerOutput(\n                prev_sample=prev_sample, pred_original_sample=pred_original_sample\n            ),\n            log_prob,\n        )\n    \n    def step_forward_logprob(\n        self,\n        model_output,\n        timestep,\n        sample,\n        next_sample,\n        eta = 1.0,\n        use_clipped_model_output = False,\n        generator=None,\n        variance_noise = None,\n        return_dict = True,):  # pylint: disable=g-bare-generic\n        \n        \"\"\"Predict the sample at the previous timestep by reversing the SDE.\n\n        Core function to propagate the diffusion process from the learned model\n        outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`): direct output from learned diffusion\n                model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`): current instance of sample (x_t) being\n                created by diffusion process.\n            next_sample (`torch.FloatTensor`): instance of next sample (x_t-1) being\n              created by diffusion process. RL sampling is the backward process,\n              therefore, x_t-1 is the \"next\" sample of x_t.\n            eta (`float`): weight of noise for added noise in diffusion step.\n            use_clipped_model_output (`bool`): if `True`, compute \"corrected\"\n                `model_output` from the clipped predicted original sample. Necessary\n                because predicted original sample is clipped to [-1, 1] when\n                `self.config.clip_sample` is `True`. If no clipping has happened,\n                \"corrected\" `model_output` would coincide with the one provided as\n                input and `use_clipped_model_output` will have not effect.\n            generator: random number generator.\n            variance_noise (`torch.FloatTensor`): instead of generating noise for\n                the variance using `generator`, we can directly provide the noise for\n                the variance itself. This is useful for methods such as\n                CycleDiffusion. (https://arxiv.org/abs/2210.05559)\n            return_dict (`bool`): option for returning tuple rather than\n                DDIMSchedulerOutput class\n\n        Returns:\n        log probability.\n        \"\"\"\n        if self.num_inference_steps is None:\n            raise ValueError(\n                \"Number of inference steps is 'None', you need to run 'set_timesteps'\"\n                \" after creating the scheduler\"\n            )\n\n        # pylint: disable=line-too-long\n        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n        # Ideally, read DDIM paper in-detail understanding\n\n        # Notation (<variable name> -> <name in paper>\n        # - pred_noise_t -> e_theta(x_t, t)\n        # - pred_original_sample -> f_theta(x_t, t) or x_0\n        # - std_dev_t -> sigma_t\n        # - eta -> η\n        # - pred_sample_direction -> \"direction pointing to x_t\"\n        # - pred_prev_sample -> \"x_t-1\"\n\n        # 1. get previous step value (=t-1)\n        prev_timestep = (\n            timestep - self.config.num_train_timesteps // self.num_inference_steps\n        )\n\n        # 2. compute alphas, betas\n        alpha_prod_t = self.alphas_cumprod.to(timestep.device)[timestep]\n        mask_a = (prev_timestep >= 0).int().to(timestep.device)\n        mask_b = 1 - mask_a\n        alpha_prod_t_prev = (\n            self.alphas_cumprod.to(timestep.device)[prev_timestep] * mask_a\n            + self.final_alpha_cumprod.to(timestep.device) * mask_b\n        )\n        beta_prod_t = 1 - alpha_prod_t\n\n        # 3. compute predicted original sample from predicted noise also called\n        # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        if self.config.prediction_type == \"epsilon\":\n            pred_original_sample = (\n                sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n        elif self.config.prediction_type == \"sample\":\n            pred_original_sample = model_output\n        elif self.config.prediction_type == \"v_prediction\":\n            pred_original_sample = (alpha_prod_t**0.5) * sample - (\n              beta_prod_t**0.5) * model_output\n            # predict V\n            model_output = (alpha_prod_t**0.5) * model_output + (\n              beta_prod_t**0.5) * sample\n        else:\n            raise ValueError(\n              f\"prediction_type given as {self.config.prediction_type} must be one\"\n              \" of `epsilon`, `sample`, or `v_prediction`\"\n            )\n\n        # 4. Clip \"predicted x_0\"\n        if self.config.clip_sample:\n            pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n\n        # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n        # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n        variance = self._get_variance_logprob(timestep, prev_timestep).to(dtype=sample.dtype)\n        std_dev_t = (eta * variance ** (0.5)).to(dtype=sample.dtype)\n\n        if use_clipped_model_output:\n            # the model_output is always re-derived from the clipped x_0 in Glide\n            model_output = (\n                sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n\n        # pylint: disable=line-too-long\n        # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (\n            0.5) * model_output\n\n        # pylint: disable=line-too-long\n        # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n        prev_sample = (\n            alpha_prod_t_prev ** (0.5) * pred_original_sample\n            + pred_sample_direction\n        )\n\n        if eta > 0:\n            device = model_output.device\n            if variance_noise is not None and generator is not None:\n                raise ValueError(\n                    \"Cannot pass both generator and variance_noise. Please make sure\"\n                    \" that either `generator` or `variance_noise` stays `None`.\"\n                )\n                \n            if variance_noise is None:\n                variance_noise = randn_tensor(\n                    model_output.shape,\n                    generator=generator,\n                    device=device,\n                    dtype=model_output.dtype)\n            variance = std_dev_t * variance_noise\n            dist = Normal(prev_sample, std_dev_t)\n            log_prob = (\n                dist.log_prob(next_sample.detach().clone())\n                .mean(dim=-1)\n                .mean(dim=-1)\n                .mean(dim=-1))\n\n        return log_prob\n        ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.108249Z","iopub.execute_input":"2024-11-05T19:41:30.108651Z","iopub.status.idle":"2024-11-05T19:41:30.163887Z","shell.execute_reply.started":"2024-11-05T19:41:30.108603Z","shell.execute_reply":"2024-11-05T19:41:30.163025Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class StableDiffusionPipelineExtended(StableDiffusionPipeline):\n    \"\"\"Extension of diffusers.StableDiffusionPipeline.\"\"\"\n\n    def forward_collect_traj_ddim(\n        self,\n        prompt = None,\n        height = None,\n        width = None,\n        num_inference_steps = 50,\n        guidance_scale = 7.5,\n        negative_prompt = None,\n        num_images_per_prompt = 1,\n        eta = 1.0,\n        generator = None,\n        latents = None,\n        prompt_embeds = None,\n        negative_prompt_embeds = None,\n        output_type = None,\n        return_dict = True,\n        callback = None,\n        callback_steps = 1,\n        cross_attention_kwargs = None,\n        is_ddp = False,\n        unet_copy = None,\n        soft_reward = False):\n        \n        # 0. Default height and width to unet\n        if is_ddp:\n            height = (height or self.unet.module.config.sample_size * self.vae_scale_factor)\n            width = (width or self.unet.module.config.sample_size * self.vae_scale_factor)\n        else:\n            height = height or self.unet.config.sample_size * self.vae_scale_factor\n            width = width or self.unet.config.sample_size * self.vae_scale_factor\n        \n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds\n        )\n            \n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n            \n        device = self._execution_device\n        do_classifier_free_guidance = guidance_scale > 1.0\n        \n        # 3. Encode input prompt\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds\n        )\n        \n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device = device)\n        timesteps = self.scheduler.timesteps\n        \n        # 5. Prepare latent variables\n        if is_ddp:\n            num_channels_latents = self.unet.module.in_channels\n        else:\n            num_channels_latents = self.unet.in_channels\n        \n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents\n        )\n        \n        latents_list = []\n        log_prob_list = []\n        latents_list.append(latents.detach().clone().cpu())\n        \n        # 6. Prepare extra step kwargs.\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n        \n        # 7. Denoising loop\n        num_warmup_steps = (\n            len(timesteps) - num_inference_steps * self.scheduler.order)\n        kl_list = []\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = (\n                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                )\n                \n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states = prompt_embeds,\n                    cross_attention_kwargs = cross_attention_kwargs,\n                ).sample\n                \n                if soft_reward:\n                    old_noise_pred = unet_copy(\n                        latent_model_input,\n                        t,\n                        encoder_hidden_states = prompt_embeds,\n                        cross_attention_kwargs=cross_attention_kwargs,\n                    ).sample\n                    \n                    if do_classifier_free_guidance:\n                        old_noise_pred_uncond, old_noise_pred_text = old_noise_pred.chunk(2)\n                        old_noise_pred = old_noise_pred_uncond + guidance_scale * (\n                            old_noise_pred_text - old_noise_pred_uncond\n                        )\n                        \n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n                    \n                # now we get the predicted noise\n                unsqueeze3x = lambda x: x[Ellipsis, None, None, None]\n                unet_t = unsqueeze3x(torch.tensor([t])).to(noise_pred.device)\n                # compute the previous noisy sample x_t -> x_t-1\n                if soft_reward:\n                    prev_latents = latents.clone()\n                    \n                latents, log_prob = self.scheduler.step_logprob(\n                    noise_pred, unet_t, latents, **extra_step_kwargs\n                )\n                latents = latents.prev_sample\n                latents = latents.to(prompt_embeds.dtype)\n                latents_list.append(latents.detach().clone().cpu())\n                log_prob_list.append(log_prob.detach().clone().cpu())\n                \n                if soft_reward:\n                    unet_times = unsqueeze3x(torch.tensor([t] * noise_pred.shape[0])).to(\n                        noise_pred.device\n                    )\n                    old_log_prob = (\n                        self.scheduler.step_forward_logprob(\n                            noise_pred,\n                            unet_times,\n                            prev_latents,\n                            latents,\n                            **extra_step_kwargs\n                        ).detach().cpu()\n                    )\n                    \n                    kl_list.append((log_prob - old_log_prob).detach().clone().cpu())\n                \n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n                        \n        if output_type == \"latent\":\n            image = latents\n        elif output_type == \"pil\":\n            # 8. Post-processing\n            latents = latents.detach()\n            latents = latents.to(prompt_embeds.dtype)\n            image = self.decode_latents(latents)\n            # 10. Convert to PIL\n            image = self.numpy_to_pil(image)\n        else:\n            image = self.decode_latents(latents)\n        \n        # Offload last model to CPU\n        if (\n            hasattr(self, \"final_offload_hook\")\n            and self.final_offload_hook is not None):\n            self.final_offload_hook.offload()\n            \n        unconditional_prompt_embeds, guided_prompt_embeds = prompt_embeds.chunk(2)\n        \n        if soft_reward:\n            kl_sum = 0\n            kl_path = []\n            for i in range(len(kl_list)):\n                kl_sum += kl_list[i]\n            kl_path.append(kl_sum.clone())\n            for i in range(1, len(kl_list)):\n                kl_sum -= kl_list[i - 1]\n                kl_path.append(kl_sum.clone())\n        else:\n            kl_path = None\n\n        return (\n            image,\n            latents_list,\n            unconditional_prompt_embeds.detach().cpu(),\n            guided_prompt_embeds.detach().cpu(),\n            log_prob_list,\n            kl_path,\n        )\n    \n    # Feed transitions pairs and old model\n    def forward_calculate_logprob(\n        self,\n        prompt_embeds,\n        latents,\n        next_latents,\n        ts,\n        unet_copy=None,\n        height = None,\n        width = None,\n        num_inference_steps = 50,\n        guidance_scale = 7.5,\n        negative_prompt = None,\n        num_images_per_prompt = 1,\n        eta = 1.0,\n        generator = None,\n        negative_prompt_embeds = None,\n        output_type = \"pil\",\n        return_dict = True,\n        callback = None,\n        callback_steps = 1,\n        cross_attention_kwargs = None,\n        is_ddp = False,\n        soft_reward=False):\n        # pylint: disable=line-too-long\n    \n        if soft_reward:\n            unet_copy = None\n        # 0. Default height and width to unet\n        if is_ddp:\n            height = (\n              height or self.unet.module.config.sample_size * self.vae_scale_factor\n            )\n            width = (\n              width or self.unet.module.config.sample_size * self.vae_scale_factor\n            )\n        else:\n            height = height or self.unet.config.sample_size * self.vae_scale_factor\n            width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 2. Define call parameters\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of\n        # equation (2) of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf .\n        # `guidance_scale = 1` corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n        unet_times = timesteps[ts]\n\n        # 4. Prepare latent variables\n        latents_list = []\n        latents_list.append(latents.detach().clone())\n        # 5. Prepare extra step kwargs.\n        # TODO: Logic should ideally just be moved out of the pipeline  # pylint: disable=g-bad-todo\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 6. Denoising loop\n        # for training loops:\n        # with self.progress_bar(total=num_inference_steps) as progress_bar:\n        # for i, t in enumerate(timesteps):\n        # expand the latents if we are doing classifier free guidance\n        latent_model_input = (\n            torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n        )\n        time_model_input = (\n            torch.cat([unet_times] * 2)\n            if do_classifier_free_guidance\n            else unet_times\n        )\n\n        # predict the noise residual\n        noise_pred = self.unet(\n            latent_model_input,\n            time_model_input,\n            encoder_hidden_states=prompt_embeds,\n            cross_attention_kwargs=cross_attention_kwargs,\n        ).sample\n\n        # add regularization\n\n        if unet_copy is not None:\n            old_noise_pred = unet_copy(\n              latent_model_input,\n              time_model_input,\n              encoder_hidden_states=prompt_embeds,\n              cross_attention_kwargs=cross_attention_kwargs,\n            ).sample\n\n        # perform guidance\n        if do_classifier_free_guidance:\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            noise_pred = noise_pred_uncond + guidance_scale * (\n              noise_pred_text - noise_pred_uncond\n            )\n\n        if unet_copy is not None:\n            if do_classifier_free_guidance:\n                old_noise_pred_uncond, old_noise_pred_text = old_noise_pred.chunk(2)\n                old_noise_pred = old_noise_pred_uncond + guidance_scale * (\n                    old_noise_pred_text - old_noise_pred_uncond\n                )\n            # now we get the predicted noise\n            kl_regularizer = (noise_pred - old_noise_pred) ** 2\n        else:\n            kl_regularizer = torch.zeros(noise_pred.shape[0])\n\n        unsqueeze3x = lambda x: x[Ellipsis, None, None, None]\n        unet_times = unsqueeze3x(unet_times).to(noise_pred.device)\n        log_prob = self.scheduler.step_forward_logprob(\n            noise_pred, unet_times, latents, next_latents, **extra_step_kwargs\n        )\n\n        return log_prob, kl_regularizer","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.165337Z","iopub.execute_input":"2024-11-05T19:41:30.165652Z","iopub.status.idle":"2024-11-05T19:41:30.210932Z","shell.execute_reply.started":"2024-11-05T19:41:30.165620Z","shell.execute_reply":"2024-11-05T19:41:30.209928Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ConditionalLinear(nn.Module):\n    \"\"\"Conditional linear.\"\"\"\n    \n    def __init__(self, num_in, num_out, n_steps):\n        super(ConditionalLinear, self).__init__()\n        self.num_out = num_out\n        self.lin = nn.Linear(num_in, num_out)\n        self.embed = nn.Embedding(n_steps, num_out)\n        self.embed.weight.data.uniform_()\n        torch.nn.init.xavier_normal_(self.lin.weight)\n        \n    def forward(self, x, y):\n        out = self.lin(x)\n        gamma = self.embed(y)\n        out = gamma.view(-1, self.num_out) * out\n        return out\n    \nclass ValueMulti(nn.Module):\n    \"\"\"ValueMulti.\"\"\"\n    \n    def __init__(self, num_steps, img_shape):\n        super(ValueMulti, self).__init__()\n        self.lin1 = ConditionalLinear(int(np.prod(img_shape)) + 768, 256, num_steps)\n        self.lin2 = ConditionalLinear(256, 256, num_steps)\n        self.lin3 = ConditionalLinear(256, 256, num_steps)\n        self.lin4 = nn.Linear(256, 1)\n        torch.nn.init.xavier_normal_(self.lin4.weight)\n        \n    def forward(self, img, txt_emb, t):\n        x = img.view(img.shape[0], -1)\n        x = torch.cat([x, txt_emb], dim = 1)\n        x = F.relu(self.lin1(x, t))\n        x = F.relu(self.lin2(x, t))\n        x = F.relu(self.lin3(x, t))\n        return self.lin4(x)\n\ndef image_reward_get_reward(\n    model, pil_image, prompt, weight_dtype):\n    \"\"\"Gets rewards using ImageReward model.\"\"\"\n    image = (model.preprocess(pil_image).unsqueeze(0).to(weight_dtype).to(model.device))\n    image_embeds = model.blip.visual_encoder(image)\n    image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(model.device)\n    text_input = model.blip.tokenizer(\n        prompt,\n        padding = \"max_length\",\n        truncation = True,\n        max_length = 35,\n        return_tensors = \"pt\",\n    ).to(model.device)\n    text_output = model.blip.text_encoder(\n        text_input.input_ids,\n        attention_mask = text_input.attention_mask,\n        encoder_hidden_states = image_embeds,\n        encoder_attention_mask = image_atts,\n        return_dict = True,\n    )\n    txt_features = text_output.last_hidden_state[:, 0, :]\n    rewards = model.mlp(txt_features)\n    rewads = (rewards - model.mean) / model.std\n    return rewards, txt_features\n\ndef _calculate_reward_ir(\n    pipe,\n    args,\n    reward_tokenizer,\n    tokenizer,\n    weight_dtype,\n    reward_clip_model,\n    image_reward,\n    imgs,\n    prompts,\n    test_flag = False\n):\n    \"\"\"Computes reward using ImageReward model.\"\"\"\n    if test_flag:\n        image_pil = imgs\n    else:\n        image_pil = pipe.numpy_to_pil(imgs)[0]\n    \n    blip_reward, _ = image_reward_get_reward(\n        image_reward, image_pil, prompts, weight_dtype\n    )\n    inputs = reward_tokenizer(\n        prompts,\n        max_length = tokenizer.model_max_length,\n        padding = \"do_not_pad\",\n        truncation = True,\n    )\n    input_ids = inputs.input_ids\n    padded_tokens = reward_tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n    txt_emb = reward_clip_model.get_text_features(input_ids=padded_tokens.input_ids.to(\"cuda\").unsqueeze(0))\n    return blip_reward.cpu().squeeze(0).squeeze(0), txt_emb.squeeze(0)\n    \ndef _collect_rollout(args, pipe, is_ddp, batch, calculate_reward, state_dict):\n    \"\"\"Collects trajectories.\"\"\"\n    for _ in range(args.g_step):\n        with torch.no_grad():\n            (\n                image,\n                latents_list,\n                unconditional_prompt_embeds,\n                guided_prompt_embeds,\n                log_prob_list,\n                _,\n            ) = pipe.forward_collect_traj_ddim(prompt=batch, is_ddp = is_ddp)\n            reward_list = []\n            txt_emb_list = []\n            for i in range(len(batch)):\n                reward, txt_emb = calculate_reward(image[i], batch[i])\n                reward_list.append(reward)\n                txt_emb_list.append(txt_emb)\n            reward_list = torch.stack(reward_list).detach().cpu()\n            txt_emb_list = torch.stack(txt_emb_list).detach().cpu()\n            # store the roll out data\n            for i in range(len(latents_list) - 1):\n                # deal with a batch of data in each step i\n                state_dict[\"prompt\"].extend(batch)\n                state_dict[\"state\"] = torch.cat((state_dict[\"state\"], latents_list[i]))\n                state_dict[\"next_state\"] = torch.cat(\n                    (state_dict[\"next_state\"], latents_list[i + 1])\n                )\n                state_dict[\"timestep\"] = torch.cat(\n                    (state_dict[\"timestep\"], torch.LongTensor([i] * len(batch)))\n                )\n                state_dict[\"final_reward\"] = torch.cat(\n                    (state_dict[\"final_reward\"], reward_list)\n                )\n                state_dict[\"unconditional_prompt_embeds\"] = torch.cat((\n                    state_dict[\"unconditional_prompt_embeds\"],\n                    unconditional_prompt_embeds,\n                ))\n                state_dict[\"guided_prompt_embeds\"] = torch.cat(\n                    (state_dict[\"guided_prompt_embeds\"], guided_prompt_embeds)\n                )\n                state_dict[\"txt_emb\"] = torch.cat((state_dict[\"txt_emb\"], txt_emb_list))\n                state_dict[\"log_prob\"] = torch.cat(\n                    (state_dict[\"log_prob\"], log_prob_list[i])\n                )\n            del (\n                image,\n                latents_list,\n                unconditional_prompt_embeds,\n                guided_prompt_embeds,\n                reward_list,\n                txt_emb_list,\n                log_prob_list,\n                reward,\n                txt_emb,\n            )\n            torch.cuda.empty_cache()\n            \ndef _trim_buffer(buffer_size, state_dict):\n    \"\"\"Delete old samples from the bufffer.\"\"\"\n    if state_dict[\"state\"].shape[0] > buffer_size:\n        state_dict[\"prompt\"] = state_dict[\"prompt\"][-buffer_size:]\n        state_dict[\"state\"] = state_dict[\"state\"][-buffer_size:]\n        state_dict[\"next_state\"] = state_dict[\"next_state\"][-buffer_size:]\n        state_dict[\"timestep\"] = state_dict[\"timstep\"][-buffer_size:]\n        state_dict[\"final_reward\"] = state_dict[\"final_reward\"][-buffer_size:]\n        state_dict[\"unconditional_prompt_embeds\"] = state_dict[\n            \"unconditional_prompt_embeds\"\n        ][-buffer_size:]\n        state_dict[\"guided_prompt_embeds\"] = state_dict[\"guided_prompt_embeds\"][\n            -buffer_size:\n        ]\n        state_dict[\"txt_emb\"] = state_dict[\"txt_emb\"][-buffer_size:]\n        state_dict[\"log_prob\"] = state_dict[\"log_prob\"][-buffer_size:]\n        \ndef _get_batch(data_iter_loader, data_iterator, prompt_list, args, accelerator):\n    \"\"\"creates a batch\"\"\"\n    batch = next(data_iter_loader, None)\n    if batch is None:\n        batch = next(\n            iter(\n                accelerator.prepare(\n                    data_iterator(prompt_list, batch_size = args.g_batch_size)\n                )\n            )\n        )\n    if args.single_flag == 1:\n        for i in range(len(batch)):\n            batch[i] = args.single_prompt\n    batch_list = []\n    for i in range(len(batch)):\n        batch_list.extend([batch[i] for _ in range(args.num_samples)])\n    batch = batch_list\n    return batch\n    \n        \ndef _save_model(args, count, is_ddp, accelerator, unet):\n    \"\"\"Saves UNET model.\"\"\"\n    save_path = os.path.join(args.output_dir, f\"save_{count}\")\n    print(f\"Saving model to {save_path}\")\n    if is_ddp:\n        unet_to_save = copy.deepcopy(accelerator.unwrap_model(unet)).to(torch.float32)\n        unet_to_save.save_attn_procs(save_path)\n    else:\n        unet_to_save = copy.deepcopy(unet).to(torch.float32)\n        unet_to_save.save_attn_procs(save_path)\n        \ndef get_random_indices(num_indices, sample_size):\n    \"\"\"Returns a random sample of indices from a larger list of indices.\"\"\"\n    if num_indices < sample_size:\n        sample_size = num_indices\n    return np.random.choice(num_indices, size=sample_size, replace = False)\n\ndef _train_value_func(value_function, state_dict, accelerator, args):\n    \"\"\"Trains the value function.\"\"\"\n    indices = get_random_indices(state_dict[\"state\"].shape[0], args.v_batch_size)\n    batch_state = state_dict[\"state\"][indices]\n    batch_timestep = state_dict[\"timestep\"][indices]\n    batch_final_reward = state_dict[\"final_reward\"][indices]\n    batch_txt_emb = state_dict[\"txt_emb\"][indices]\n    pred_value = value_function(\n        batch_state.cuda().detach(),\n        batch_txt_emb.cuda().detach(),\n        batch_timestep.cuda().detach()\n    )\n    batch_final_reward = batch_final_reward.cuda().float()\n    value_loss = F.mse_loss(\n      pred_value.float().reshape([args.v_batch_size, 1]),\n      batch_final_reward.cuda().detach().reshape([args.v_batch_size, 1]))\n    accelerator.backward(value_loss / args.v_step)\n    del (\n        pred_value,\n        batch_state,\n        batch_timestep,\n        batch_final_reward,\n        batch_txt_emb\n    )\n    return (value_loss.item() / args.v_step)\n\ndef _train_policy_func(\n    args,\n    state_dict,\n    pipe,\n    unet_copy,\n    is_ddp,\n    count,\n    policy_steps,\n    accelerator,\n    tpfdata,\n    value_function\n):\n    \"\"\"Trains the policy function.\"\"\"\n    with torch.no_grad():\n        indices = get_random_indices(state_dict[\"state\"].shape[0], args.p_batch_size)\n        batch_state = state_dict[\"state\"][indices]\n        batch_next_state = state_dict[\"next_state\"][indices]\n        batch_timestep = state_dict[\"timestep\"][indices]\n        batch_final_reward = state_dict[\"final_reward\"][indices]\n        batch_unconditional_prompt_embeds = state_dict[\"unconditional_prompt_embeds\"][indices]\n        batch_guided_prompt_embeds = state_dict[\"guided_prompt_embeds\"][indices]\n        batch_prompt_embeds = torch.cat(\n            [batch_unconditional_prompt_embeds, batch_guided_prompt_embeds]\n        )\n        batch_txt_emb = state_dict[\"txt_emb\"][indices]\n        batch_log_prob = state_dict[\"log_prob\"][indices]\n        \n    # calculate loss from the custom function\n    log_prob, kl_regularizer = pipe.forward_calculate_logprob(\n        prompt_embeds = batch_prompt_embeds.cuda(),\n        latents = batch_state.cuda(),\n        next_latents = batch_next_state.cuda(),\n        ts = batch_timestep.cuda(),\n        unet_copy = unet_copy,\n        is_ddp = is_ddp,\n    )\n    with torch.no_grad():\n        adv = batch_final_reward.cuda().reshape([args.p_batch_size, 1]) - value_function(\n            batch_state.cuda(),\n            batch_txt_emb.cuda(),\n            batch_timestep.cuda()\n        ).reshape([args.p_batch_size, 1])\n    ratio = torch.exp(log_prob - batch_log_prob.cuda())\n    ratio = torch.clamp(ratio, 1.0 - args.ratio_clip, 1.0 + args.ratio_clip)\n    loss = (\n        -args.reward_weight\n        * adv.detach().float()\n        * ratio.float().reshape([args.p_batch_size, 1])\n    ).mean()\n    if count > args.kl_warmup:\n        loss += args.kl_weight * kl_regularizer.mean()\n    loss = loss / (args.gradient_accumulation_steps)\n    accelerator.backward(loss)\n    # logging\n    tpfdata.tot_ratio += ratio.mean().item() / policy_steps\n    tpfdata.tot_kl += kl_regularizer.mean().item() / policy_steps\n    tpfdata.tot_p_loss += loss.item() / policy_steps\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.212432Z","iopub.execute_input":"2024-11-05T19:41:30.212942Z","iopub.status.idle":"2024-11-05T19:41:30.267316Z","shell.execute_reply.started":"2024-11-05T19:41:30.212903Z","shell.execute_reply":"2024-11-05T19:41:30.266348Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def _update_output_dir(args):\n    \"\"\"Modifies `args.output_dir` using configurations in `args`.\n    \n    Args:\n        args: argparse.Namespace object.\n    \"\"\"\n    if args.single_flag == 1:\n        data_log = \"single_prompt/\" + args.single_prompt.replace(\" \", \"_\") + \"/\"\n    else:\n        data_log = args.prompt_path.split(\"/\")[-2] + \"_\"\n        data_log += args.prompt_category \n    learning_log = \"p_lr\" + str(args.learning_rate) + \"_s\" + str(args.p_step)\n    learning_log += (\n        \"_b\"\n        + str(args.p_batch_size)\n        + \"_g\"\n        + str(args.gradient_accumulation_steps)\n    )\n    learning_log += \"_l\" + str(args.lora_rank)\n    coeff_log = \"_kl\" + str(args.kl_weight) + \"_re\" + str(args.reward_weight)\n    if args.kl_warmup > 0:\n        coeff_log += \"_klw\" + str(args.kl_warmup)\n    if args.sft_initialization == 0:\n        start_log = \"pre_train/\"\n    else:\n        start_log = \"/sft/\"\n    if args.reward_flag == 0:\n        args.output_dir += \"/img_reward_{}/\".format(args.reward_filter)\n    else:\n        args.output_dir += \"/pregreward_{}/\".format(args.reward_filter)\n    args.output_dir += start_log + data_log + \"/\" + learning_log + coeff_log\n    if args.v_flag == 1:\n        value_log = \"_v_lr\" + str(args.v_lr) + \"_b\" + str(args.v_batch_size)\n        value_log += \"_s\" + str(args.v_step)\n        args.output_dir += value_log\n    return args","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.268613Z","iopub.execute_input":"2024-11-05T19:41:30.269178Z","iopub.status.idle":"2024-11-05T19:41:30.281370Z","shell.execute_reply.started":"2024-11-05T19:41:30.269142Z","shell.execute_reply":"2024-11-05T19:41:30.280563Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train_model(orig_args):\n    # Update output paths for logging purposes.\n    args = _update_output_dir(orig_args)\n    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n    accelerator_project_config = ProjectConfiguration(\n        logging_dir = logging_dir, total_limit = 2\n    )\n    accelerator = Accelerator(\n        mixed_precision = args.mixed_precision,\n        log_with = args.report_to,\n        project_config = accelerator_project_config\n    )\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n    \n    # Handle the repository creation\n    if accelerator.is_main_process:\n        os.makedirs(args.output_dir, exist_ok=True)\n    # Load scheduler, tokenizer and models.\n    tokenizer = CLIPTokenizer.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"tokenizer\",\n        revision=args.revision\n    )\n    text_encoder = CLIPTextModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=args.revision,\n    )\n    weight_dtype = torch.float16\n    \n    # reward models\n    reward_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n    reward_processor = CLIPProcessor.from_pretrained(\n        \"openai/clip-vit-large-patch14\"\n    )\n    reward_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n    \n    if args.reward_flag == 0:\n        image_reward = imagereward.load(\"ImageReward-v1.0\")\n    else:\n        reward_model = pickle.load(open(args.reward_model_path, \"rb\"))[\"reward\"]\n    image_reward.requires_grad_(False)\n    image_reward.to(accelerator.device, dtype=weight_dtype)\n    reward_clip_model.requires_grad_(False)\n    \n    pipe = StableDiffusionPipelineExtended.from_pretrained(\n        args.pretrained_model_name_or_path, torch_dtype=weight_dtype\n    )\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder = \"unet\",\n        revision = args.revision\n    )\n    pipe.scheduler = DDIMSchedulerExtended.from_config(pipe.scheduler.config)\n    vae = pipe.vae\n    unet.requires_grad_(False)\n    unet.eval()\n    text_encoder = pipe.text_encoder\n    \n    # Freeze vae and text_encoder\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    # pretrain model to calculate kl\n    unet_copy = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"unet\",\n        revision=args.revision,\n    )\n    # freeze unet copy\n    unet_copy.requires_grad_(False)\n    # Move text_encode and vae to gpu and cast to weight_dtype\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\n    vae.to(accelerator.device, dtype=weight_dtype)\n    unet.to(accelerator.device, dtype=weight_dtype)\n    unet_copy.to(accelerator.device, dtype=weight_dtype)\n    \n    # Define lora layers\n    lora_attn_procs = {}\n    for name in unet.attn_processors.keys():\n        cross_attention_dim = (\n            None\n            if name.endswith(\"attn1.processor\")\n            else unet.config.cross_attention_dim\n        )\n        if name.startswith(\"mid_block\"):\n            hidden_size = unet.config.block_out_channels[-1]\n        elif name.startswith(\"up_block\"):\n            block_id =  int(name[len(\"up_blocks.\")])\n            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n        elif name.startswith(\"down_block\"):\n            block_id = int(name[len(\"down_blocks.\")])\n            hidden_size = unet.config.block_out_channels[block_id]\n        \n        lora_attn_procs[name] = LoRACrossAttnProcessor(\n            hidden_size = hidden_size,\n            cross_attention_dim = cross_attention_dim,\n            rank = args.lora_rank\n        )\n        \n    unet.set_attn_processor(lora_attn_procs)\n    lora_layers = AttnProcsLayers(unet.attn_processors)\n    \n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n    \n    unet.enable_gradient_checkpointing()\n    \n    # Initialize the optimizer\n    optimizer_cls = torch.optim.AdamW\n    \n    optimizer = optimizer_cls(\n        lora_layers.parameters(),\n        lr = args.learning_rate,\n        betas = (args.adam_beta1, args.adam_beta2),\n        weight_decay = args.adam_weight_decay,\n        eps = args.adam_epsilon,\n    )\n    \n    with open(args.prompt_path) as json_file:\n        prompt_dict = json.load(json_file)\n    \n    prompt_list = []\n    for prompt in prompt_dict:\n        category = prompt_dict[prompt][\"category\"]\n        prompt_list.append(prompt)\n    \n    # Data iterator for prompt dataset\n    def _my_data_iterator(data, batch_size):\n        # Shuffle the data randomly\n        random.shuffle(data)\n        \n        for i in range(0, len(data), batch_size):\n            batch = data[i : i + batch_size]\n            yield batch\n    \n    data_iterator = _my_data_iterator(prompt_list, batch_size = args.g_batch_size)\n    data_iterator = accelerator.prepare(data_iterator)\n    \n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer = optimizer,\n        num_warmup_steps = args.lr_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps = args.max_train_steps * args.gradient_accumulation_steps,\n    )\n    \n    value_function = ValueMulti(50, (4, 64, 64))\n    value_optimizer = torch.optim.AdamW(value_function.parameters(), lr = args.v_lr)\n    value_function, value_optimizer = accelerator.prepare(value_function, value_optimizer)\n    \n    lora_layers, optimizer, lr_scheduler = accelerator.prepare(lora_layers, optimizer, lr_scheduler)\n    \n    reward_clip_model.to(accelerator.device, dtype=weight_dtype)\n    \n    if accelerator.is_main_process:\n        accelerator.init_trackers(\"text2image-fine-tune\", config = vars(args))\n        \n    # Train!\n    total_batch_size = (\n        args.train_batch_size\n        * accelerator.num_processes\n        * args.gradient_accumulation_steps\n    )\n    global_step = 0\n    \n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(\n      f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(\n      \"  Total train batch size (w. parallel, distributed & accumulation) =\"\n      f\" {total_batch_size}\")\n    logger.info(\n      f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(\n      f\"  Total optimization steps = {args.max_train_steps // args.p_step}\")\n    \n    pipe.scheduler = DDIMSchedulerExtended.from_config(pipe.scheduler.config)\n    pipe.to(\"cuda\")\n    \n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(\n        range(global_step, args.max_train_steps),\n        disable=not accelerator.is_local_main_process,\n    )\n    progress_bar.set_description(\"Steps\")\n\n    def _map_cpu(x):\n        return x.cpu()\n    \n    state_dict = {}\n    state_dict[\"prompt\"] = []\n    state_dict[\"state\"] = _map_cpu(torch.FloatTensor().to(weight_dtype))\n    state_dict[\"next_state\"] = _map_cpu(torch.FloatTensor().to(weight_dtype))\n    state_dict[\"timestep\"] = _map_cpu(torch.LongTensor())\n    state_dict[\"final_reward\"] = _map_cpu(torch.FloatTensor().to(weight_dtype))\n    state_dict[\"unconditional_prompt_embeds\"] = _map_cpu(\n          torch.FloatTensor().to(weight_dtype)\n    )\n    state_dict[\"guided_prompt_embeds\"] = _map_cpu(\n        torch.FloatTensor().to(weight_dtype)\n    )\n    state_dict[\"txt_emb\"] = _map_cpu(torch.FloatTensor().to(weight_dtype))\n    state_dict[\"log_prob\"] = _map_cpu(torch.FloatTensor().to(weight_dtype))\n    \n    calculate_reward = functools.partial(\n        _calculate_reward_ir,\n        pipe,\n        args,\n        reward_tokenizer,\n        tokenizer,\n        weight_dtype,\n        reward_clip_model,\n        image_reward\n    )\n    \n    count = 0\n    buffer_size = args.buffer_size\n    policy_steps = args.gradient_accumulation_steps * args.p_step\n    data_iter_loader = iter(data_iterator)\n    is_ddp = isinstance(unet, DistributedDataParallel)\n    pipe.unet = unet\n    print(\"model is parallel:\", is_ddp)\n    \n    for count in range(0, args.max_train_steps // args.p_step):\n        # fix batchnorm\n        unet.eval()\n        \n        batch = _get_batch(\n            data_iter_loader, _my_data_iterator, prompt_list, args, accelerator\n        )\n        _collect_rollout(args, pipe, is_ddp, batch, calculate_reward, state_dict)\n        _trim_buffer(buffer_size, state_dict)\n        \n        total_val_loss = 0\n        value_optimizer.zero_grad()\n        for v_step in range(args.v_step):\n            if v_step < args.v_step - 1:\n                with accelerator.no_sync(value_function):\n                    total_val_loss += _train_value_func(\n                        value_function, state_dict, accelerator, args\n                    )\n            else:\n                total_val_loss += _train_value_func(\n                    value_function, state_dict, accelerator, args\n                )\n        value_optimizer.step()\n        value_optimizer.zero_grad()\n        if accelerator.is_local_main_process:\n            print(\"value_loss\", total_val_loss)\n            accelerator.log({\"value_loss\": total_val_loss}, step=count)\n        del total_val_loss\n        torch.cuda.empty_cache()\n        \n        # policy learning\n        tpfdata = TrainPolicyFuncData()\n        for _ in range(args.p_step):\n            optimizer.zero_grad()\n            for accum_step in range(int(args.gradient_accumulation_steps)):\n                if accum_step < int(args.gradient_accumulation_steps) - 1:\n                    with accelerator.no_sync(unet):\n                        _train_policy_func(\n                            args,\n                            state_dict,\n                            pipe,\n                            unet_copy,\n                            is_ddp,\n                            count,\n                            policy_steps,\n                            accelerator,\n                            tpfdata,\n                            value_function\n                        )\n                else:\n                    _train_policy_func(\n                        args,\n                        state_dict,\n                        pipe,\n                        unet_copy,\n                        is_ddp,\n                        count,\n                        policy_steps,\n                        accelerator,\n                        tpfdata,\n                        value_function\n                    )\n            \n            if accelerator.sync_gradients:\n                norm = accelerator.clip_grad_norm_(unet.parameters(), args.clip_norm)\n            tpfdata.tot_grad_norm += norm.item() / args.p_step\n            optimizer.step()\n            lr_scheduler.step()\n            if accelerator.is_main_process:\n                print(f\"count: [{count} / {args.max_train_steps // args.p_step}]\")\n                print(\"train_reward\", torch.mean(state_dict[\"final_reward\"]).item())\n                accelerator.log(\n                    {\"train_reward\": torch.mean(state_dict[\"final_reward\"]).item()},\n                    step=count,\n                )\n                print(\"grad norm\", tpfdata.tot_grad_norm, \"ratio\", tpfdata.tot_ratio)\n                print(\"kl\", tpfdata.tot_kl, \"p_loss\", tpfdata.tot_p_loss)\n                accelerator.log({\"grad norm\": tpfdata.tot_grad_norm}, step=count)\n                accelerator.log({\"ratio\": tpfdata.tot_ratio}, step=count)\n                accelerator.log({\"kl\": tpfdata.tot_kl}, step=count)\n                accelerator.log({\"p_loss\": tpfdata.tot_p_loss}, step=count)\n            torch.cuda.empty_cache()\n            \n        if accelerator.sync_gradients:\n            global_step += 1\n            if global_step % args.checkpointing_steps == 0:\n                if accelerator.is_main_process:\n                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                    accelerator.save_state(output_dir=save_path)\n                    logger.info(f\"Saved state to {save_path}\")\n            print(\"global_step\", global_step)\n        \n        # save model per interval\n        if count % args.save_interval == 0:\n            accelerator.wait_for_everyone()\n            if accelerator.is_main_process:\n                _save_model(args, count, is_ddp, accelerator, unet)\n    \n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        _save_model(args, count, is_ddp, accelerator, unet)\n    \n    accelerator.end_training()","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.282601Z","iopub.execute_input":"2024-11-05T19:41:30.282948Z","iopub.status.idle":"2024-11-05T19:41:30.332284Z","shell.execute_reply.started":"2024-11-05T19:41:30.282880Z","shell.execute_reply":"2024-11-05T19:41:30.331304Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"args = Args()\ntrain_model(args)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T19:41:30.335207Z","iopub.execute_input":"2024-11-05T19:41:30.335557Z","iopub.status.idle":"2024-11-05T19:42:33.952258Z","shell.execute_reply.started":"2024-11-05T19:41:30.335517Z","shell.execute_reply":"2024-11-05T19:42:33.950414Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35583fd0e87f40dc94f25886b6a4cd91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2936e47164464855891c1879c1d2476f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c07d60eef5b4215a490c3400961912a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b238c677f784ce3a36da2c5cad93259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff536d3d61554d5c8aae1d9e56620b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"225061bc73744ed2a3fe8713d0713c73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fadf923b2747d683a6cce87ae7a390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4098899311824d59ac102e1357c950bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637748c4d8cd4b72b61b3bb9caded791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec5ab5a6c45440abf60185746af8b96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a3cbe9a96f447782306c0b96f5ae44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa38e2d404014ce98a629eaa88b3afd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e24fd0d36849cfa0031822e647463e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff41ccb0ae64e01937f6105cb51dc7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ImageReward.pt:   0%|          | 0.00/1.79G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd7cf0261004da8a6ce3c17eb0b517a"}},"metadata":{}},{"name":"stdout","text":"load checkpoint from /root/.cache/ImageReward/ImageReward.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"med_config.json:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b808a7a8f18144c883074a2916f13f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68998229081948ed9bb0197783c2bcd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533221c148e94ea5b72f658518b68c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8296063179594b69bd3de3609d356d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba75d867ddd46288a8b7b518c5d2775"}},"metadata":{}},{"name":"stdout","text":"checkpoint loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e32b26ef89b49488a8464facd160402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa92da228c9d4a8fb8c8420f3eceb7ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78d46ced4df84b78a0bf18816fd74d76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848f62950c9c4784841e68db0b787dd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8b96cc86a64d078b9929011e9390fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1cf647ef694715a4b5c32db7821fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3b803ac8ee46efb64bcebfe893b6da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1431f21ba64bf899c6a9357efd312c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"023e967349034a3a8d333e9247269406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28924eba579b42deb7d5ebae8e6cccb7"}},"metadata":{}},{"name":"stderr","text":"{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n{'scaling_factor'} was not found in config. Values will be initialized to default values.\n{'prediction_type'} was not found in config. Values will be initialized to default values.\n{'time_embedding_act_fn', 'num_class_embeds', 'upcast_attention', 'resnet_time_scale_shift', 'conv_in_kernel', 'mid_block_type', 'time_embedding_type', 'only_cross_attention', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'timestep_post_act', 'encoder_hid_dim', 'resnet_out_scale_factor', 'class_embed_type', 'time_embedding_dim', 'class_embeddings_concat', 'resnet_skip_time_act', 'cross_attention_norm', 'dual_cross_attention', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'conv_out_kernel', 'mid_block_only_cross_attention', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n{'time_embedding_act_fn', 'num_class_embeds', 'upcast_attention', 'resnet_time_scale_shift', 'conv_in_kernel', 'mid_block_type', 'time_embedding_type', 'only_cross_attention', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'timestep_post_act', 'encoder_hid_dim', 'resnet_out_scale_factor', 'class_embed_type', 'time_embedding_dim', 'class_embeddings_concat', 'resnet_skip_time_act', 'cross_attention_norm', 'dual_cross_attention', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'conv_out_kernel', 'mid_block_only_cross_attention', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n{'thresholding', 'sample_max_value', 'dynamic_thresholding_ratio', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n{'time_embedding_act_fn', 'num_class_embeds', 'upcast_attention', 'resnet_time_scale_shift', 'conv_in_kernel', 'mid_block_type', 'time_embedding_type', 'only_cross_attention', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'timestep_post_act', 'encoder_hid_dim', 'resnet_out_scale_factor', 'class_embed_type', 'time_embedding_dim', 'class_embeddings_concat', 'resnet_skip_time_act', 'cross_attention_norm', 'dual_cross_attention', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'conv_out_kernel', 'mid_block_only_cross_attention', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7278e3c3b7409ea5341aeb89817b32"}},"metadata":{}},{"name":"stdout","text":"model is parallel: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d1daa270034907b601be089c40860b"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 245\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(orig_args)\u001b[0m\n\u001b[1;32m    240\u001b[0m unet\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    242\u001b[0m batch \u001b[38;5;241m=\u001b[39m _get_batch(\n\u001b[1;32m    243\u001b[0m     data_iter_loader, _my_data_iterator, prompt_list, args, accelerator\n\u001b[1;32m    244\u001b[0m )\n\u001b[0;32m--> 245\u001b[0m \u001b[43m_collect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ddp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m _trim_buffer(buffer_size, state_dict)\n\u001b[1;32m    248\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","Cell \u001b[0;32mIn[7], line 105\u001b[0m, in \u001b[0;36m_collect_rollout\u001b[0;34m(args, pipe, is_ddp, batch, calculate_reward, state_dict)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mg_step):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     98\u001b[0m         (\n\u001b[1;32m     99\u001b[0m             image,\n\u001b[1;32m    100\u001b[0m             latents_list,\n\u001b[1;32m    101\u001b[0m             unconditional_prompt_embeds,\n\u001b[1;32m    102\u001b[0m             guided_prompt_embeds,\n\u001b[1;32m    103\u001b[0m             log_prob_list,\n\u001b[1;32m    104\u001b[0m             _,\n\u001b[0;32m--> 105\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_collect_traj_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ddp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_ddp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m         reward_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    107\u001b[0m         txt_emb_list \u001b[38;5;241m=\u001b[39m []\n","Cell \u001b[0;32mIn[6], line 108\u001b[0m, in \u001b[0;36mStableDiffusionPipelineExtended.forward_collect_traj_ddim\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, is_ddp, unet_copy, soft_reward)\u001b[0m\n\u001b[1;32m    103\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    104\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance \u001b[38;5;28;01melse\u001b[39;00m latents\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soft_reward:\n\u001b[1;32m    116\u001b[0m     old_noise_pred \u001b[38;5;241m=\u001b[39m unet_copy(\n\u001b[1;32m    117\u001b[0m         latent_model_input,\n\u001b[1;32m    118\u001b[0m         t,\n\u001b[1;32m    119\u001b[0m         encoder_hidden_states \u001b[38;5;241m=\u001b[39m prompt_embeds,\n\u001b[1;32m    120\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m    121\u001b[0m     )\u001b[38;5;241m.\u001b[39msample\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:724\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m downsample_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(downsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m downsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 724\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:867\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m--> 867\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    873\u001b[0m     output_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/transformer_2d.py:265\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 265\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention.py:313\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    310\u001b[0m     norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(hidden_states)\n\u001b[1;32m    312\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 313\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monly_cross_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm_zero:\n\u001b[1;32m    320\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention_processor.py:267\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention_processor.py:474\u001b[0m, in \u001b[0;36mLoRAAttnProcessor.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, scale)\u001b[0m\n\u001b[1;32m    471\u001b[0m key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(key)\n\u001b[1;32m    472\u001b[0m value \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(value)\n\u001b[0;32m--> 474\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_probs, value)\n\u001b[1;32m    476\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mbatch_to_head_dim(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention_processor.py:308\u001b[0m, in \u001b[0;36mAttention.get_attention_scores\u001b[0;34m(self, query, key, attention_mask)\u001b[0m\n\u001b[1;32m    305\u001b[0m     baddbmm_input \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m    306\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 308\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaddbmm_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupcast_softmax:\n\u001b[1;32m    317\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39mfloat()\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 15.89 GiB of which 4.11 GiB is free. Process 2946 has 11.77 GiB memory in use. Of the allocated memory 11.23 GiB is allocated by PyTorch, and 255.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 15.89 GiB of which 4.11 GiB is free. Process 2946 has 11.77 GiB memory in use. Of the allocated memory 11.23 GiB is allocated by PyTorch, and 255.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}