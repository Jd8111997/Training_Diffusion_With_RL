{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9994246,"sourceType":"datasetVersion","datasetId":6151234}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diffusers[torch]>=0.29.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:04:38.483147Z","iopub.execute_input":"2024-11-29T18:04:38.483618Z","iopub.status.idle":"2024-11-29T18:04:49.712678Z","shell.execute_reply.started":"2024-11-29T18:04:38.483584Z","shell.execute_reply":"2024-11-29T18:04:49.711465Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install peft>=0.6.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:04:49.715014Z","iopub.execute_input":"2024-11-29T18:04:49.715276Z","iopub.status.idle":"2024-11-29T18:04:58.485352Z","shell.execute_reply.started":"2024-11-29T18:04:49.715252Z","shell.execute_reply":"2024-11-29T18:04:58.484217Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Clone the ImageReward repository (containing data for testing)\n!git clone https://github.com/THUDM/ImageReward.git\n!cd ImageReward\n\n# Install the integrated package `image-reward`\n!pip install image-reward\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:04:58.487160Z","iopub.execute_input":"2024-11-29T18:04:58.487955Z","iopub.status.idle":"2024-11-29T18:05:39.029692Z","shell.execute_reply.started":"2024-11-29T18:04:58.487910Z","shell.execute_reply":"2024-11-29T18:05:39.028823Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ImageReward'...\nremote: Enumerating objects: 224, done.\u001b[K\nremote: Counting objects: 100% (91/91), done.\u001b[K\nremote: Compressing objects: 100% (49/49), done.\u001b[K\nremote: Total 224 (delta 52), reused 49 (delta 41), pack-reused 133 (from 1)\u001b[K\nReceiving objects: 100% (224/224), 4.30 MiB | 34.64 MiB/s, done.\nResolving deltas: 100% (89/89), done.\nCollecting image-reward\n  Downloading image_reward-1.5-py3-none-any.whl.metadata (12 kB)\nCollecting timm==0.6.13 (from image-reward)\n  Downloading timm-0.6.13-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: transformers>=4.27.4 in /opt/conda/lib/python3.10/site-packages (from image-reward) (4.45.1)\nCollecting fairscale==0.4.13 (from image-reward)\n  Downloading fairscale-0.4.13.tar.gz (266 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.13.4 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.25.1)\nRequirement already satisfied: diffusers>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.31.0)\nRequirement already satisfied: accelerate>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (0.34.2)\nRequirement already satisfied: datasets>=2.11.0 in /opt/conda/lib/python3.10/site-packages (from image-reward) (3.0.1)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from fairscale==0.4.13->image-reward) (2.4.0)\nRequirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from fairscale==0.4.13->image-reward) (1.26.4)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.6.13->image-reward) (0.19.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.6.13->image-reward) (6.0.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (5.9.3)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.16.0->image-reward) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.11.0->image-reward) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.11.0->image-reward) (3.9.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (7.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (2024.5.15)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.16.0->image-reward) (10.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.4->image-reward) (4.12.2)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.27.4->image-reward) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.11.0->image-reward) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.16.0->image-reward) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.11.0->image-reward) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->fairscale==0.4.13->image-reward) (3.1.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers>=0.16.0->image-reward) (3.19.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.11.0->image-reward) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.11.0->image-reward) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->fairscale==0.4.13->image-reward) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->fairscale==0.4.13->image-reward) (1.3.0)\nDownloading image_reward-1.5-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.6.13-py3-none-any.whl (549 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fairscale\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332107 sha256=d8ea6350a5b663ba9f4832cdad890839167f8c259a0063018abb076b2b6b6f5d\n  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\nSuccessfully built fairscale\nInstalling collected packages: fairscale, timm, image-reward\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\nSuccessfully installed fairscale-0.4.13 image-reward-1.5 timm-0.6.13\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-btkn3pd2\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-btkn3pd2\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=961430049ff5f33afea8961b019044190c8ca6be0e39fe52c159fe4bfc9b83fb\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5_id5mdq/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Importing modules","metadata":{}},{"cell_type":"code","source":"import logging\nimport torch\nimport contextlib\nimport random\nfrom functools import partial\nimport math\nimport ImageReward as RM\nimport os\nimport wandb\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Union, Callable\nimport tempfile\n\nfrom peft import LoraConfig\nfrom peft.utils import get_peft_model_state_dict\nimport csv\nimport collections\nimport numpy as np\nimport functools\nimport time\nimport tqdm\ntqdm = partial(tqdm.tqdm, dynamic_ncols=True)\n\nfrom PIL import Image\nimport torch.nn as nn\nfrom torchvision.transforms import Compose, Resize, CenterCrop, Normalize\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC\n\n\nimport diffusers\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import cast_training_params\nfrom diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available\nfrom diffusers.utils.torch_utils import is_compiled_module\nfrom diffusers.models.embeddings import TimestepEmbedding, Timesteps, GaussianFourierProjection\nfrom diffusers.models.unets.unet_2d_blocks import get_down_block, DownBlock2D, CrossAttnDownBlock2D\n\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import (\n    StableDiffusionPipeline,\n    rescale_noise_cfg,\n)\ntry:\n    from diffusers.utils import randn_tensor\nexcept ImportError:\n    from diffusers.utils.torch_utils import randn_tensor\n\nfrom diffusers.schedulers.scheduling_ddim import DDIMSchedulerOutput, DDIMScheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:39.031783Z","iopub.execute_input":"2024-11-29T18:05:39.032072Z","iopub.status.idle":"2024-11-29T18:05:57.165994Z","shell.execute_reply.started":"2024-11-29T18:05:39.032045Z","shell.execute_reply":"2024-11-29T18:05:57.165232Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"@dataclass()\nclass Config:\n    save_freq = 10\n    num_checkpoint_limit = 3\n    mixed_precision = \"bf16\"\n    allow_tf32 = True\n    use_lora = True\n    pretrained = {\n        \"model\": \"runwayml/stable-diffusion-v1-5\",\n        \"revision\": \"main\",\n    }\n    sample = {\n        \"num_steps\": 50,\n        \"eta\": 1.0,\n        \"guidance_scale\": 5.0,\n        \"batch_size\": 2,\n        \"num_batches_per_epoch\": 2,\n    }\n    train = {\n        \"use_8bit_adam\": False,\n        \"learning_rate\": 3.0e-4,\n        \"adam_beta1\": .9,\n        \"adam_beta2\": .999,\n        \"adam_weight_decay\": 1.0e-4,\n        \"adam_epsilon\": 1.0e-8,\n        \"max_grad_norm\": 1.0,\n        \"num_inner_epochs\": 1,\n        \"cfg\": True,\n        \"adv_clip_max\": 5,\n        \"clip_range\": 1.0e-4,\n        \"timestep_fraction\": 1.0,\n        \"lora_rank\": 4,\n        \"batch_size\": 2,\n        \"gradient_accumulation_steps\": 2,\n        \"reward_exp\": 1.0e+2,\n        \"flow_learning_rate\": 3.0e-4,\n        \"anneal\": \"linear\",\n        \"unetreg\": 1.0e+0,\n        \"klpf\": -1,\n    }\n    seed = 0\n    num_epochs = 100\n    wandb = False\n    prompt_fn = \"drawbench\"\n    reward_fn = \"imagereward\"\n    prompt_fn_kwargs = { }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.166938Z","iopub.execute_input":"2024-11-29T18:05:57.167447Z","iopub.status.idle":"2024-11-29T18:05:57.173926Z","shell.execute_reply.started":"2024-11-29T18:05:57.167419Z","shell.execute_reply":"2024-11-29T18:05:57.172920Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_seed(seed):\n    import random\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    import numpy as np\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.empty_cache()\n\n    logging.info(f'Using seed: {seed}')\n\ndef image_postprocess(x):\n    return torch.clamp((x + 1) / 2, 0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.175246Z","iopub.execute_input":"2024-11-29T18:05:57.175640Z","iopub.status.idle":"2024-11-29T18:05:57.201320Z","shell.execute_reply.started":"2024-11-29T18:05:57.175601Z","shell.execute_reply":"2024-11-29T18:05:57.200578Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/alignment-gflownet-assets/assets\"\n\n@functools.lru_cache()\ndef read_csv(path):\n    with open(path, \"r\") as f:\n        reader = csv.DictReader(f)\n        reader = [row for row in reader]\n\n    info = collections.defaultdict(list)\n    for row in reader:\n        info[row[\"Category\"]].append(row[\"Prompts\"])\n    filtered_info = {}\n    for k, v in info.items():\n        if k in [\"Misspellings\", \"Rare Words\"]:\n            continue\n        filtered_info[k] = v[2:]\n    drawbench_prompt_ls = sum(filtered_info.values(), [])\n    return drawbench_prompt_ls\n\ndef drawbench():\n    drawbench_prompt_ls = read_csv(os.path.join(BASE_PATH, \"DrawBench Prompts.csv\" ))\n    return random.choice(drawbench_prompt_ls), {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.202242Z","iopub.execute_input":"2024-11-29T18:05:57.202479Z","iopub.status.idle":"2024-11-29T18:05:57.210643Z","shell.execute_reply.started":"2024-11-29T18:05:57.202456Z","shell.execute_reply":"2024-11-29T18:05:57.209808Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def imagereward(dtype=torch.float32, device=\"cuda\"):\n    reward_model = RM.load(\"ImageReward-v1.0\")\n    reward_model.to(dtype).to(device)\n\n    rm_preprocess = Compose([\n        Resize(224, interpolation=BICUBIC),\n        CenterCrop(224),\n        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n    ])\n\n    def _fn(images, prompts, metadata):\n        dic = reward_model.blip.tokenizer(prompts,\n                padding=\"max_length\", truncation=True,\n                return_tensors=\"pt\", max_length=reward_model.blip.tokenizer.model_max_length)\n        device = images.device\n        input_ids, attention_mask = dic.input_ids.to(device), dic.attention_mask.to(device)\n        reward = reward_model.score_gard(input_ids, attention_mask, rm_preprocess(images))\n        return reward.reshape(images.shape[0]).float(), {}\n\n    return _fn\n\ndef unwrap_model(model):\n    model = model._orig_mod if is_compiled_module(model) else model\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.211590Z","iopub.execute_input":"2024-11-29T18:05:57.211852Z","iopub.status.idle":"2024-11-29T18:05:57.223314Z","shell.execute_reply.started":"2024-11-29T18:05:57.211827Z","shell.execute_reply":"2024-11-29T18:05:57.222439Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ConditionalFlow(torch.nn.Module):\n    def __init__(self,\n        # sample_size: Optional[int] = None,\n        in_channels: int = 4,\n        # center_input_sample: bool = False,\n        flip_sin_to_cos: bool = True,\n        freq_shift: int = 0,\n        down_block_types: Tuple[str] = (\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n        only_cross_attention: Union[bool, Tuple[bool]] = False,\n        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n        layers_per_block: Union[int, Tuple[int]] = 2,\n        downsample_padding: int = 1,\n        act_fn: str = \"silu\",\n        norm_num_groups: Optional[int] = 32,\n        norm_eps: float = 1e-5,\n        cross_attention_dim: Union[int, Tuple[int]] = 1280,\n        encoder_hid_dim: Optional[int] = None,\n        encoder_hid_dim_type: Optional[str] = None,\n        attention_head_dim: Union[int, Tuple[int]] = 8,\n        timestep_post_act: Optional[str] = None,\n        time_cond_proj_dim: Optional[int] = None,\n        conv_in_kernel: int = 3,\n        class_embeddings_concat: bool = False):\n\n        super().__init__()\n        timestep_input_dim = block_out_channels[0]\n        self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos = flip_sin_to_cos, downscale_freq_shift = freq_shift)\n        time_embed_dim = block_out_channels[0] * 4\n        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim, act_fn=act_fn,\n                                               post_act_fn=timestep_post_act, cond_proj_dim=time_cond_proj_dim)\n        conv_in_padding = (conv_in_kernel - 1) // 2\n        self.conv_in = nn.Conv2d(\n            in_channels, block_out_channels[0], kernel_size = conv_in_kernel, padding = conv_in_padding\n        )\n        self.encoder_hid_proj = None\n        self.down_blocks = nn.ModuleList([])\n        if isinstance(attention_head_dim, int):\n            attention_head_dim = (attention_head_dim,) * len(down_block_types)\n\n        if isinstance(cross_attention_dim, int):\n            cross_attention_dim = (cross_attention_dim,) * len(down_block_types)\n\n        if isinstance(layers_per_block, int):\n            layers_per_block = [layers_per_block] * len(down_block_types)\n\n        if class_embeddings_concat:\n            blocks_time_embed_dim = time_embed_dim * 2\n        else:\n            blocks_time_embed_dim = time_embed_dim\n\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n\n            down_block = get_down_block(\n                    down_block_type,\n                    num_layers=layers_per_block[i],\n                    in_channels=input_channel,\n                    out_channels=output_channel,\n                    temb_channels=blocks_time_embed_dim,\n                    # add_downsample=not is_final_block,\n                    add_downsample=True,\n                    resnet_eps=norm_eps,\n                    resnet_act_fn=act_fn,\n                    resnet_groups=norm_num_groups,\n                    cross_attention_dim=cross_attention_dim[i],\n                    # attn_num_head_channels=attention_head_dim[i], # old diffusers version\n                    num_attention_heads=attention_head_dim[i],\n                    attention_head_dim=attention_head_dim[i], # can be annotated\n                    downsample_padding=downsample_padding,\n            )\n            self.down_blocks.append(down_block)\n\n        self.pool = nn.AvgPool2d(4, stride = 4)\n        self.fc = nn.Linear(block_out_channels[-1], 1)\n\n\n    def forward(self, sample, timesteps, encoder_hidden_states, attention_mask: Optional[torch.Tensor] = None,\n               cross_attention_kwargs: Optional[Dict[str, Any]] = None, encoder_attention_mask: Optional[torch.Tensor] = None,):\n\n        dtype = next(self.down_blocks.parameters()).dtype\n        t_emb = self.time_proj(timesteps)\n        t_emb = t_emb.to(dtype=dtype)\n        emb = self.time_embedding(t_emb)\n\n        sample = self.conv_in(sample)\n        for downsample_block in self.down_blocks:\n            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n                sample, res_samples = downsample_block(\n                    hidden_states = sample,\n                    temb = emb,\n                    encoder_hidden_states=encoder_hidden_states,\n                    attention_mask=attention_mask,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                    encoder_attention_mask=encoder_attention_mask,\n                )\n            else:\n                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n        sample = self.pool(sample)\n        sample = sample.view(sample.size(0), -1)\n        sample = self.fc(sample).squeeze()\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.224626Z","iopub.execute_input":"2024-11-29T18:05:57.224935Z","iopub.status.idle":"2024-11-29T18:05:57.238779Z","shell.execute_reply.started":"2024-11-29T18:05:57.224893Z","shell.execute_reply":"2024-11-29T18:05:57.238007Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def _left_broadcast(t, shape):\n    assert t.ndim <= len(shape)\n    return t.reshape(t.shape + (1,) * (len(shape) - t.ndim)).broadcast_to(shape)\n\ndef _get_variance(self, timestep, prev_timestep):\n    alpha_prod_t = torch.gather(self.alphas_cumprod, 0, timestep.cpu()).to(timestep.device)\n    alpha_prod_t_prev = torch.where(\n        prev_timestep.cpu() >= 0,\n        self.alphas_cumprod.gather(0, prev_timestep.cpu()),\n        self.final_alpha_cumprod,\n    ).to(timestep.device)\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n    return variance\n\ndef ddim_step_with_logprob(\n    self: DDIMScheduler,\n    model_output: torch.FloatTensor,\n    timestep: int,\n    sample: torch.FloatTensor,\n    eta: float = 1.0,\n    use_clipped_model_output: bool = False,\n    generator = None,\n    prev_sample: Optional[torch.FloatTensor] = None,\n    calculate_pb: bool = False,\n    logp_mean = True,\n    prev_timestep: int = None,\n) -> Union[DDIMSchedulerOutput, Tuple]:\n    \n    assert isinstance(self, DDIMScheduler)\n    if self.num_inference_steps is None:\n        raise ValueEror(\n            \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n        )\n\n    # 1. get previous step value (=t-1)\n    if prev_timestep is None:\n        prev_timestep = (\n            timestep - self.config.num_train_timesteps // self.num_inference_steps\n        )\n    prev_timestep = torch.clamp(prev_timestep, 0, self.config.num_train_timesteps - 1)\n\n    # 2. compute alphas, betas\n    alpha_prod_t = self.alphas_cumprod.gather(0, timestep.cpu())\n    alpha_prod_t_prev = torch.where(\n        prev_timestep.cpu() >= 0,\n        self.alphas_cumprod.gather(0, prev_timestep.cpu()),\n        self.final_alpha_cumprod,\n    )\n    alpha_prod_t = _left_broadcast(alpha_prod_t, sample.shape).to(sample.device)\n    alpha_prod_t_prev = _left_broadcast(alpha_prod_t_prev, sample.shape).to(sample.device)\n    beta_prod_t = 1 - alpha_prod_t\n    \n    # 3. compute predicted original sample from predicted noise also called\n    if self.config.prediction_type == \"epsilon\":\n        pred_original_sample = (\n            sample - beta_prod_t ** (0.5) * model_output\n        ) / alpha_prod_t ** 0.5\n        pred_epsilon = model_output\n    elif self.config.prediction_type == \"sample\":\n        pred_original_sample = model_output\n        pred_epsilon = (\n            sample - alpha_prod_t ** (0.5) * pred_original_sample\n        ) / beta_prod_t ** 0.5\n    elif self.config.prediction_type == \"v_prediction\":\n        pred_original_sample = (alpha_prod_t ** 0.5) * sample - (\n            beta_prod_t ** 0.5\n        ) * model_output\n        pred_epsilon = (alpha_prod_t**0.5) * model_output + (\n            beta_prod_t**0.5\n        ) * sample\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n            \" `v_prediction`\"\n        )\n    # 4. Clip or threshold \"predicted x_0\"\n    if self.config.thresholding:\n        pred_original_sample = self._threshold_sample(pred_original_sample)\n    elif self.config.clip_sample:\n        pred_original_sample = pred_original_sample.clamp(\n            -self.config.clip_sample_range, self.config.clip_sample_range\n        )\n\n    # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n    variance = _get_variance(self, timestep, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n    std_dev_t = _left_broadcast(std_dev_t, sample.shape).to(sample.device)\n\n    if use_clipped_model_output:\n        pred_epsilon = (\n            sample - alpha_prod_t ** (0.5) * pred_original_sample\n        ) / (beta_prod_t) ** 0.5\n\n    # 6. compute \"direction pointing to x_t\" of formula (12)\n    prev_sample_direction = (1 - alpha_prod_t_prev - std_dev_t ** 2) ** (0.5) * pred_epsilon \n\n    # 7. compute x_t without \"random noise\" \n    prev_sample_mean = (alpha_prod_t_prev ** (0.5) * pred_original_sample + prev_sample_direction)\n\n    if prev_sample is not None and generator is not None:\n        raise ValueError(\n            \"Cannot pass both generator and prev_sample. Please make sure that either `generator` or\"\n            \" `prev_sample` stays `None`.\"\n        )\n    if prev_sample is None:\n        variance_noise = randn_tensor(\n            model_output.shape,\n            generator = generator,\n            device = model_output.device,\n            dtype = model_output.dtype,\n        )\n        prev_sample = prev_sample_mean + std_dev_t * variance_noise\n\n    log_prob = (\n        -((prev_sample.detach() - prev_sample_mean) ** 2) / (2 * (std_dev_t ** 2))\n        - torch.log(std_dev_t)\n        - torch.log(torch.sqrt(2 * torch.as_tensor(math.pi)))\n    )\n\n    if logp_mean:\n        log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))\n    else:\n        log_prob = log_prob.sum(dim=tuple(range(1, log_prob.ndim)))\n\n    if calculate_pb:\n        assert prev_sample is not None\n        alpha_ddim = alpha_prod_t / alpha_prod_t_prev  # (bs, 4, 64, 64)\n        pb_mean = alpha_ddim.sqrt() * prev_sample\n        pb_std = (1 - alpha_ddim).sqrt()\n        log_pb = (\n                -((sample.detach() - pb_mean.detach()) ** 2) / (2 * (pb_std ** 2))\n                - torch.log(pb_std)\n                - torch.log(torch.sqrt(2 * torch.as_tensor(math.pi)))\n        )\n        if logp_mean:\n            log_pb = log_pb.mean(dim=tuple(range(1, sample.ndim)))\n        else:\n            log_pb = log_pb.sum(dim=tuple(range(1, sample.ndim)))\n        return prev_sample.type(sample.dtype), log_prob, log_pb\n\n    else:\n        return prev_sample.type(sample.dtype), log_prob\n\n@torch.no_grad\ndef pred_orig_latent(self: DDIMScheduler, model_output, sample: torch.FloatTensor, timestep: int):\n    alpha_prod_t = self.alphas_cumprod.gather(0, timestep.cpu())\n    alpha_prod_t = _left_broadcast(alpha_prod_t, sample.shape).to(sample.device)\n    alpha_prod_t = alpha_prod_t.to(sample.dtype)\n    beta_prod_t = 1 - alpha_prod_t\n\n    if self.config.prediction_type == \"epsilon\":\n        pred_original_sample = (\n            sample - beta_prod_t ** (0.5) * model_output\n        ) / alpha_prod_t ** (0.5)\n    elif self.config.prediction_type == \"sample\":\n        pred_original_sample = model_output\n    elif self.config.prediction_type == \"v_prediction\":\n        pred_original_sample = (alpha_prod_t**0.5) * sample - (\n            beta_prod_t**0.5\n        ) * model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n            \" `v_prediction`\"\n        )\n    return pred_original_sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.241851Z","iopub.execute_input":"2024-11-29T18:05:57.242069Z","iopub.status.idle":"2024-11-29T18:05:57.262856Z","shell.execute_reply.started":"2024-11-29T18:05:57.242048Z","shell.execute_reply":"2024-11-29T18:05:57.261732Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"@torch.no_grad()\ndef pipeline_with_logprob(\n    self: StableDiffusionPipeline,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    guidance_rescale: float = 0.0,\n\n    batch_size = None, dtype=None,\n    device = None,\n    calculate_pb = False, logp_mean = True,\n    return_unetoutput = False,\n):\n    \n    # 0. Default height and width to unet\n    if height is None:\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n    if width is None:\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n    # 1. Check inputs. Raise error if not correct\n    if hasattr(self, \"check_inputs\"):  # DDPMPipeline does not have this method\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n        )\n\n    # 2. Define call parameters\n    if batch_size is None:\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n    if device is None:\n        device = self._execution_device\n    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n    # corresponds to doing no classifier free guidance.\n    do_classifier_free_guidance = guidance_scale > 1.0\n\n    # 3. Encode input prompt\n    if prompt_embeds is not None:\n        text_encoder_lora_scale = (\n            cross_attention_kwargs.get(\"scale\", None)\n            if cross_attention_kwargs is not None\n            else None\n        )\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            lora_scale=text_encoder_lora_scale,\n        )\n\n    # 4. Prepare timesteps\n    if num_inference_steps is None:\n        timesteps = self.scheduler.timesteps\n        num_inference_steps = len(timesteps)\n    else:\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n    # 5. Prepare latent variables\n    num_channels_latents = self.unet.config.in_channels\n    if prompt_embeds is not None:\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta) # {'eta': 1.0, 'generator': None}\n\n    else:\n        shape = (batch_size, num_channels_latents, height, width)\n        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        extra_step_kwargs = {'eta': eta, 'generator': generator}\n\n    # 7. Denoising loop\n    # self.scheduler.order is 1, not sure what it does\n    num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n\n    all_latents = [latents]\n    all_log_probs = []\n    all_log_pbs = []\n    unet_outputs = []\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = (\n                torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            )\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # predict the noise residual\n            if prompt_embeds is not None:\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                    return_dict=False,\n                )[0]\n            else:\n                noise_pred = self.unet(\n                    latent_model_input, t, return_dict=False\n                )[0]\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (\n                    noise_pred_text - noise_pred_uncond\n                )\n            if return_unetoutput:\n                unet_outputs.append(noise_pred.detach())\n\n            # by default not used (as guidance_rescale = 0.0)\n            if do_classifier_free_guidance and guidance_rescale > 0.0:\n                # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                noise_pred = rescale_noise_cfg(\n                    noise_pred, noise_pred_text, guidance_rescale=guidance_rescale\n                )\n\n            # compute the previous noisy sample x_t -> x_t-1\n            prev_timestep = timesteps[i + 1] if i < num_inference_steps-1 else None\n            if calculate_pb:\n                latents, log_prob, log_pb = ddim_step_with_logprob(\n                    self.scheduler, noise_pred, t, latents,\n                    calculate_pb=calculate_pb, logp_mean=logp_mean,\n                    prev_timestep=prev_timestep, #\n                    **extra_step_kwargs\n                )\n                all_log_pbs.append(log_pb)\n            else:\n                latents, log_prob = ddim_step_with_logprob(\n                    self.scheduler, noise_pred, t, latents,\n                    prev_timestep=prev_timestep, #\n                    **extra_step_kwargs\n                )\n\n            all_latents.append(latents)\n            all_log_probs.append(log_prob)\n\n            # call the callback, if provided\n            if i == len(timesteps) - 1 or (\n                (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n            ):\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n\n    if not output_type == \"latent\":\n        image = self.vae.decode(\n            latents / self.vae.config.scaling_factor, return_dict=False\n        )[0]\n        image, has_nsfw_concept = self.run_safety_checker(\n            image, device, prompt_embeds.dtype\n        )\n    else:\n        image = latents\n        has_nsfw_concept = None\n\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n    # At least for the cifar10 DDPM, the generated image is in [-1, 1],\n    # so we need this postprocessing to make it [0, 1]\n    if prompt_embeds is not None:\n        image = self.image_processor.postprocess(\n            image, output_type=output_type, do_denormalize=do_denormalize\n        )\n        # Offload last model to CPU\n        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n            self.final_offload_hook.offload()\n    else:\n        # image = (image / 2 + 0.5).clamp(0, 1)\n        image = image_postprocess(image)\n\n    assert not (calculate_pb and return_unetoutput), \"Cannot return both log_pb and unet_outputs\"\n    if calculate_pb:\n        return image, has_nsfw_concept, all_latents, all_log_probs, all_log_pbs\n    if return_unetoutput:\n        return image, has_nsfw_concept, all_latents, all_log_probs, unet_outputs\n\n    return image, has_nsfw_concept, all_latents, all_log_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.264441Z","iopub.execute_input":"2024-11-29T18:05:57.264844Z","iopub.status.idle":"2024-11-29T18:05:57.291466Z","shell.execute_reply.started":"2024-11-29T18:05:57.264807Z","shell.execute_reply":"2024-11-29T18:05:57.288579Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main():\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger = logging.getLogger(__name__)\n    cfg = Config()\n    cfg.gpu_type = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n    logger.info(f\"GPU type: {cfg.gpu_type}\")\n    output_dir = os.path.join(\"./output\")\n    os.makedirs(output_dir, exist_ok=True)\n    logger.info(f\"\\n{cfg}\")\n    set_seed(cfg.seed)\n    weight_dtype = torch.float32\n    if cfg.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif cfg.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    pipeline = StableDiffusionPipeline.from_pretrained(\n        cfg.pretrained[\"model\"], revision=cfg.pretrained[\"revision\"], torch_dtype=weight_dtype,\n    )\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    scheduler_config = {}\n    scheduler_config.update(pipeline.scheduler.config)\n    pipeline.scheduler = DDIMScheduler.from_config(scheduler_config)\n    pipeline.vae.requires_grad_(False)\n    pipeline.text_encoder.requires_grad_(False)\n    pipeline.vae.to(device, dtype=weight_dtype)\n    pipeline.text_encoder.to(device, dtype=weight_dtype)\n\n    pipeline.safety_checker = None\n    pipeline.set_progress_bar_config(\n        position=1,\n        disable=False,\n        leave=False,\n        desc=\"Timestep\",\n        dynamic_ncols=True,\n    )\n\n    unet = pipeline.unet\n    unet.requires_grad_(False)\n    for param in unet.parameters():\n        param.requires_grad_(False)\n    unet.to(device, dtype=weight_dtype)\n\n    unet_lora_config = LoraConfig(\n        r=cfg.train[\"lora_rank\"], lora_alpha=cfg.train[\"lora_rank\"],\n        init_lora_weights=\"gaussian\", target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n    )\n    unet.add_adapter(unet_lora_config)\n    if cfg.mixed_precision in [\"fp16\", \"bf16\"]:\n        cast_training_params(unet, dtype=torch.float32)\n    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n    scaler = None\n    if cfg.mixed_precision in [\"fp16\", \"bf16\"]:\n        scaler = torch.cuda.amp.GradScaler()\n\n    if cfg.train[\"use_8bit_adam\"]:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n            )\n        optimizer_cls = bnb.optim.AdamW8bit\n    else:\n        optimizer_cls = torch.optim.AdamW\n\n    reward_fn = imagereward(weight_dtype, device)\n    # Generate negative prompt embeddings.\n    neg_prompt_embed = pipeline.text_encoder(\n        pipeline.tokenizer(\n            [\"\"],\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=pipeline.tokenizer.model_max_length,\n        ).input_ids.to(device)\n    )[0]\n\n    sample_neg_prompt_embeds = neg_prompt_embed.repeat(cfg.sample[\"batch_size\"], 1, 1)\n    train_neg_prompt_embeds = neg_prompt_embed.repeat(cfg.train[\"batch_size\"], 1, 1)\n\n    def func_autocast():\n        return torch.cuda.amp.autocast(dtype=weight_dtype)\n    if cfg.use_lora:\n        autocast = contextlib.nullcontext\n    else:\n        autocast = func_autocast\n\n    unet.to(device)\n\n    def decode(latents):\n        image = pipeline.vae.decode(\n            latents / pipeline.vae.config.scaling_factor, return_dict=False\n        )[0]\n        do_denormalize = [True] * image.shape[0]\n        image = pipeline.image_processor.postprocess(image,\n                    output_type = \"pt\", do_denormalize=do_denormalize\n                )\n        return image\n\n    flow_model = ConditionalFlow(in_channels = 4, block_out_channels=(64, 128, 256, 256),\n                                layers_per_block=1, cross_attention_dim=pipeline.text_encoder.config.hidden_size)\n    flow_model = flow_model.to(device, dtype=torch.float32)\n    autocast_flow = func_autocast\n    params = [\n        {\"params\": lora_layers, \"lr\": cfg.train[\"learning_rate\"]},\n        {\"params\": flow_model.parameters(), \"lr\": cfg.train[\"learning_rate\"]}\n    ]\n    optimizer = optimizer_cls(\n        params,\n        betas = (cfg.train[\"adam_beta1\"], cfg.train[\"adam_beta2\"]),\n        weight_decay = cfg.train[\"adam_weight_decay\"],\n        eps = cfg.train[\"adam_epsilon\"],\n    )\n    result = collections.defaultdict(dict)\n    result[\"config\"] = cfg\n    start_time = time.time()\n    #######################################################\n    samples_per_epoch = (\n        cfg.sample[\"batch_size\"] * cfg.sample[\"num_batches_per_epoch\"]\n    )\n    total_train_batch_size = (\n        cfg.train[\"batch_size\"] * cfg.train[\"gradient_accumulation_steps\"]\n    )\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num Epochs = {cfg.num_epochs}\")\n    logger.info(f\"  Sample batch size per device = {cfg.sample['batch_size']}\")\n    logger.info(f\"  Train batch size per device = {cfg.train['batch_size']}\")\n    logger.info(\n        f\"  Gradient Accumulation steps = {cfg.train['gradient_accumulation_steps']}\"\n    )\n    logger.info(\"\")\n    logger.info(f\"  Total number of samples per epoch = test_bs * num_batch_per_epoch = {samples_per_epoch}\")\n    logger.info(\n        f\"  Total train batch size = train_bs * grad_accumul = {total_train_batch_size}\"\n    )\n    logger.info(\n        f\"  Number of gradient updates per inner epoch = samples_per_epoch // total_train_batch_size = {samples_per_epoch // total_train_batch_size}\"\n    )\n    logger.info(f\"  Number of inner epochs = {cfg.train['num_inner_epochs']}\")\n\n    assert cfg.sample['batch_size'] >= cfg.train['batch_size']\n    assert cfg.sample['batch_size'] % cfg.train['batch_size'] == 0 # not necessary\n    assert samples_per_epoch % total_train_batch_size == 0\n\n    first_epoch = 0\n    global_step = 0\n    \n    for epoch in range(first_epoch, cfg.num_epochs):\n        if cfg.train[\"anneal\"] in [\"linear\"]:\n            ratio = min(1, epoch / (0.5 * cfg.num_epochs))\n        else:\n            ratio = 1.0\n        reward_exp_ep = cfg.train[\"reward_exp\"] * ratio\n        def reward_transform(value):\n            return value * reward_exp_ep\n\n        num_diffusion_steps = cfg.sample[\"num_steps\"]\n        pipeline.scheduler.set_timesteps(num_diffusion_steps, device = device)\n        scheduler_dt = pipeline.scheduler.timesteps[0] - pipeline.scheduler.timesteps[1]\n        num_train_timesteps = int(num_diffusion_steps * cfg.train['timestep_fraction'])\n        accumulation_steps =  cfg.train[\"gradient_accumulation_steps\"] * num_train_timesteps\n\n        torch.cuda.empty_cache()\n        unet.zero_grad()\n        unet.eval()\n        flow_model.zero_grad()\n\n        if True:\n            with torch.inference_mode():\n                samples = []\n                prompts = []\n                for i in tqdm(\n                    range(cfg.sample[\"num_batches_per_epoch\"]),\n                    desc=f\"Epoch {epoch}: sampling\",\n                    disable=False,\n                    position=0,\n                ):\n                    # generate prompts\n                    prompts, prompt_metadata = zip(\n                        *[\n                            drawbench()\n                            for _ in range(cfg.sample[\"batch_size\"])\n                        ]\n                    )\n                    # encode prompts\n                    prompt_ids = pipeline.tokenizer(\n                        prompts,\n                        return_tensors=\"pt\",\n                        padding=\"max_length\",\n                        truncation=True,\n                        max_length=pipeline.tokenizer.model_max_length,\n                    ).input_ids.to(device)\n                    prompt_embeds = pipeline.text_encoder(prompt_ids)[0]\n\n                    # sample\n                    with autocast():\n                        ret_tuple = pipeline_with_logprob(\n                            pipeline,\n                            prompt_embeds = prompt_embeds,\n                            negative_prompt_embeds = sample_neg_prompt_embeds,\n                            num_inference_steps = num_diffusion_steps,\n                            guidance_scale = cfg.sample[\"guidance_scale\"],\n                            eta = cfg.sample[\"eta\"],\n                            output_type = \"pt\",\n                            return_unetoutput = cfg.train['unetreg'] > 0.,\n                        )\n\n                    if cfg.train[\"unetreg\"] > 0:\n                        images, _, latents, log_probs, unet_outputs = ret_tuple\n                        unet_outputs = torch.stack(unet_outputs, dim = 1)\n                    else:\n                        images, _, latents, log_probs = ret_tuple\n\n                    latents = torch.stack(latents, dim = 1)\n                    log_probs = torch.stack(log_probs, dim = 1)\n                    timesteps = pipeline.scheduler.timesteps.repeat(\n                        cfg.sample[\"batch_size\"], 1\n                    )\n\n                    rewards = reward_fn(images, prompts, prompt_metadata)\n\n                    samples.append(\n                        {\n                            \"prompts\": prompts,\n                            \"prompt_metadata\": prompt_metadata,\n                            \"prompt_ids\": prompt_ids,\n                            \"prompt_embeds\": prompt_embeds,\n                            \"timesteps\": timesteps,\n                            \"latents\": latents[:, :-1],\n                            \"next_latents\": latents[:, 1:],\n                            \"log_probs\": log_probs,\n                            \"rewards\": rewards,\n                        }\n                    )\n                    if cfg.train[\"unetreg\"] > 0:\n                        samples[-1][\"unet_outputs\"] = unet_outputs\n\n\n                # wait for all rewards to be computed\n                for sample in tqdm(\n                    samples,\n                    desc = \"Waiting for rewards\",\n                    disable = False,\n                    position = 0,\n                ):\n                    rewards, reward_metadata = sample[\"rewards\"]\n                    sample[\"rewards\"] = torch.as_tensor(rewards, device=device)\n\n            # collate samples into dict where each entry has shape\n            new_samples = {}\n            for k in samples[0].keys():\n                if k in [\"prompts\", \"prompt_metadata\"]:\n                    new_samples[k] = [item for s in samples for item in s[k]]\n                else:\n                    new_samples[k] = torch.cat([s[k] for s in samples])\n            samples = new_samples\n\n            # this is a hack to force wandb to log the images as JPEGs instead of PNGs\n            with tempfile.TemporaryDirectory() as tmpdir:\n                for i, image in enumerate(images):\n                    pil = Image.fromarray(\n                        (image.cpu().float().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n                    )\n                    pil = pil.resize((256, 256))\n                    pil.save(os.path.join(tmpdir, f\"{i}.jpg\"))\n                if cfg.wandb:\n                    wandb.log(\n                        {\n                            \"images\": [\n                                wandb.Image(\n                                    os.path.join(tmpdir, f\"{i}.jpg\"),\n                                    caption=f\"{prompt} | {reward:.2f}\",\n                                )\n                                for i, (prompt, reward) in enumerate(\n                                    zip(prompts, rewards)\n                                )\n                            ],\n                        },\n                        step=global_step,\n                    )\n                    \n            \n            rewards = samples[\"rewards\"].to(dtype=samples[\"rewards\"].dtype, device=device).cpu().float().numpy()\n            result[\"reward_mean\"][global_step] = rewards.mean()\n            result[\"reward_std\"][global_step] = rewards.std()\n            logger.info(f\"global_step: {global_step}  rewards: {rewards.mean().item():.3f}\")\n            if cfg.wandb:\n                wandb.log(\n                    {\n                        \"reward_mean\": rewards.mean(),\n                        \"reward_std\": rewards.std(),\n                    },\n                    step = global_step,\n                )\n            del samples[\"prompt_ids\"]\n            total_batch_size, num_timesteps = samples[\"timesteps\"].shape\n            assert (total_batch_size == cfg.sample[\"batch_size\"] * cfg.sample[\"num_batches_per_epoch\"])\n            assert num_timesteps == num_diffusion_steps\n            \n\n            ############################## TRAINING #####################\n            for inner_epoch in range(cfg.train[\"num_inner_epochs\"]):\n                 # shuffle samples along batch dimension\n                perm = torch.randperm(total_batch_size, device = device)\n                for k, v in samples.items():\n                    if k in [\"prompts\", \"prompt_metadata\"]:\n                        samples[k] = [v[i] for i in perm]\n                    elif k in [\"unet_outputs\"]:\n                        samples[k] = v[perm]\n                    else:\n                        samples[k] = v[perm]\n\n                perms = torch.stack(\n                    [\n                        torch.randperm(num_timesteps, device = device)\n                        for _ in range(total_batch_size)\n                    ]\n                )\n                key_ls = [\"timesteps\", \"latents\", \"next_latents\", \"log_probs\"]\n                for key in key_ls:\n                     samples[key] = samples[key][torch.arange(total_batch_size, device=device)[:, None], perms]\n                if cfg.train[\"unetreg\"] > 0:\n                    samples[\"unet_outputs\"] = samples[\"unet_outputs\"][torch.arange(total_batch_size, device=device)[:, None], perms]\n\n                ### rebatch for training\n                samples_batched = {}\n                for k, v in samples.items():\n                    if k in [\"prompts\", \"prompt_metadata\"]:\n                        samples_batched[k] = [v[i: i + cfg.train[\"batch_size\"]] for i in range(0, len(v), cfg.train[\"batch_size\"])]\n                    elif k in [\"unet_outputs\"]:\n                        samples_batched[k] = v.reshape(-1, cfg.train[\"batch_size\"], *v.shape[1:])\n                    else:\n                        samples_batched[k] = v.reshape(-1, cfg.train[\"batch_size\"], *v.shape[1:])\n\n                samples_batched = [\n                    dict(zip(samples_batched, x)) for x in zip(*samples_batched.values())\n                ]\n                unet.train()\n                flow_model.train()\n                info = defaultdict(list)\n                for i, sample in tqdm(\n                    list(enumerate(samples_batched)),\n                    desc=f\"Epoch {epoch}.{inner_epoch}: training\",\n                    position=0,\n                    disable=False\n                ):\n                    torch.cuda.empty_cache()\n                    if cfg.train[\"cfg\"]:\n                        embeds = torch.cat([train_neg_prompt_embeds, sample[\"prompt_embeds\"]])\n                    else:\n                        embeds = sample[\"prompt_embeds\"]\n\n                    for j in tqdm(range(num_train_timesteps), desc=\"Timestep\", position=1, leave=False, disable=False):\n                        with autocast():\n                            if cfg.train[\"cfg\"]:\n                                noise_pred = unet(\n                                    torch.cat([sample[\"latents\"][:, j]] * 2),\n                                    torch.cat([sample[\"timesteps\"][:, j]] * 2),\n                                    embeds,\n                                ).sample\n                                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                                noise_pred = (\n                                    noise_pred_uncond + cfg.sample[\"guidance_scale\"] * (noise_pred_text - noise_pred_uncond)\n                                )\n                                if cfg.train[\"unetreg\"] > 0:\n                                    unetreg = (noise_pred - sample[\"unet_outputs\"][:, j]) ** 2\n                                    unetreg = torch.mean(unetreg, dim = (1, 2, 3))\n                            else:\n                                noise_pred = unet(\n                                    sample[\"latents\"][:, j],\n                                    sample[\"timesteps\"][:, j],\n                                    embeds,\n                                ).sample\n                                if cfg.train[\"unetreg\"] > 0:\n                                    unetreg = (noise_pred - sample[\"unet_outputs\"][:, j]) ** 2\n\n                            _, log_pf, log_pb = ddim_step_with_logprob(\n                                pipeline.scheduler, noise_pred,\n                                sample[\"timesteps\"][:, j],\n                                sample[\"latents\"][:, j],\n                                eta = cfg.sample[\"eta\"],\n                                prev_sample=sample[\"next_latents\"][:, j], calculate_pb=True,\n                            )\n\n                        with autocast_flow():\n                            flow = flow_model(sample[\"latents\"][:, j], sample[\"timesteps\"][:, j], sample[\"prompt_embeds\"])\n                            timestep_next = torch.clamp(sample[\"timesteps\"][:, j] - scheduler_dt, min=0)\n                            flow_next = flow_model(sample[\"next_latents\"][:, j], timestep_next, sample[\"prompt_embeds\"])\n\n                        with autocast(), torch.no_grad():\n                            unet_output = unet(sample[\"latents\"][:, j], sample[\"timesteps\"][:, j], sample[\"prompt_embeds\"]).sample\n                            latent = pred_orig_latent(pipeline.scheduler, unet_output, sample[\"latents\"][:, j], sample[\"timesteps\"][:, j])\n                        with torch.inference_mode():\n                            logr_temp = reward_fn(decode(latent), sample[\"prompts\"], sample[\"prompt_metadata\"])[0]\n                        logr = reward_transform(logr_temp)\n                        flow = flow + logr\n                        \n                        with autocast(), torch.no_grad():\n                            unet_output = unet(sample[\"next_latents\"][:, j], timestep_next, sample[\"prompt_embeds\"]).sample\n                            latent_next = pred_orig_latent(pipeline.scheduler, unet_output, sample[\"next_latents\"][:, j], timestep_next)\n                        with torch.inference_mode():\n                            logr_next_temp = reward_fn(decode(latent_next), sample[\"prompts\"], sample[\"prompt_metadata\"])[0]\n                        logr_next = reward_transform(logr_next_temp)\n                        flow_next = flow_next + logr_next\n\n                        info[\"log_pf\"].append(torch.mean(log_pf).detach())\n                        info[\"flow\"].append(torch.mean(flow).detach())\n                        info[\"log_pb\"].append(torch.mean(log_pb).detach())\n\n                        if cfg.train[\"klpf\"] > 0:\n                            losses_flow = (flow + log_pf.detach() - log_pb.detach() - flow_next) ** 2\n\n                            flow_next_klpf = flow_next.detach()\n                            log_pb_klpf, log_pf_klpf = log_pb.detach(), log_pf.detach()\n                            reward_db = (flow_next_klpf + log_pb_klpf - log_pf_klpf - flow).detach()\n\n                            # different gpu has different states, so cannot share a baseline\n                            assert len(reward_db) > 1\n                            rloo_baseline = (reward_db.sum() - reward_db) / (len(reward_db) - 1)\n                            reward_ = (reward_db - rloo_baseline) ** 2\n                            rloo_var = (reward_.sum() - reward_) / (len(reward_db) - 1)\n                            advantages = (reward_db - rloo_baseline) / (rloo_var.sqrt() + 1e-8)\n                            advantages = torch.clamp(advantages, -cfg['train']['adv_clip_max'], cfg['train']['adv_clip_max'])\n\n                            ratio = torch.exp(log_pf - sample[\"log_probs\"][:, j])\n                            unclipped_losses = -advantages * ratio\n                            clipped_losses = -advantages * torch.clamp(\n                                ratio,\n                                1.0 - cfg['train']['clip_range'],\n                                1.0 + cfg['train']['clip_range'],\n                            )\n                            losses_klpf = torch.maximum(unclipped_losses, clipped_losses)\n                            info[\"ratio\"].append(torch.mean(ratio).detach())\n\n                            losses = losses_flow + cfg['train']['klpf'] * losses_klpf\n                            info[\"loss\"].append(losses_flow.mean().detach())\n                            info[\"loss_klpf\"].append(losses_klpf.mean().detach())\n                            torch.cuda.empty_cache() # clear comp graph for log_pf_next\n                            \n                        else:\n                            losses_gfn = (flow + log_pf - log_pb - flow_next) ** 2\n                            info[\"loss\"].append(losses_gfn.mean().detach())\n                            losses = losses_gfn\n\n                        if cfg.train[\"unetreg\"] > 0:\n                            losses = losses + cfg.train[\"unetreg\"] * unetreg\n                            info[\"unetreg\"].append(unetreg.mean().detach())\n                        loss = torch.mean(losses)\n\n                        if logr_tmp is not None:\n                            info[\"logr\"].append(torch.mean(logr_tmp).detach())\n\n                        loss = loss / accumulation_steps\n                        if scaler:\n                            scaler.scale(loss).backward()\n                        else:\n                            loss.backward()\n\n                        # prevent OOM\n                        image_next = image = prev_sample_klpf = unet_output = latent = latent_next = latent_next_next = None\n                        noise_pred_next_uncond = noise_pred_next_text = noise_pred_uncond = noise_pred_text = noise_pred = noise_pred_next = None\n                        flow = flow_next = flow_next_next = logr = logr_next = logr_next_next = logr_next_tmp = logr_tmp = reward_db = advantages = None\n                        _ = log_pf = log_pb = log_pf_next = log_pb_next = log_pf_klpf = log_pb_klpf = None\n                        unetreg = unetreg_initial = losses = losses_flow = losses_klpf = losses_gfn = None\n\n                    if ((j == num_train_timesteps - 1) and\n                        (i + 1) % cfg.train['gradient_accumulation_steps'] == 0):\n                        if scaler:\n                            scaler.unscale_(optimizer)\n                            torch.nn.utils.clip_grad_norm_(unet.parameters(), cfg.train['max_grad_norm'])\n                            torch.nn.utils.clip_grad_norm_(flow_model.parameters(), cfg.train['max_grad_norm'])\n                            scaler.step(optimizer)\n                            scaler.update()\n                        else:\n                            torch.nn.utils.clip_grad_norm_(unet.parameters(), cfg.train['max_grad_norm'])\n                            torch.nn.utils.clip_grad_norm_(flow_model.parameters(), cfg['train']['max_grad_norm'])\n                        optimizer.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n                    info = {k: torch.mean(torch.stack(v)) for k, v in info.items()}\n                    for k, v in info.items():\n                        result[k][global_step] = v.item()\n\n                    info.update({\"epoch\": epoch})\n                    result[\"epoch\"][global_step] = epoch\n                    result[\"time\"][global_step] = time.time() - start_time\n\n                    if cfg['wandb']:\n                        wandb.log(info, step=global_step)\n                    logger.info(f\"global_step={global_step}  \" +\n                              \" \".join([f\"{k}={v:.3f}\" for k, v in info.items()]))\n                    info = defaultdict(list) # reset info dict\n\n    pickle.dump(result, gzip.open(os.path.join(output_dir, f\"result.json\"), 'wb'))\n    if epoch % cfg['save_freq'] == 0 or epoch == cfg['num_epochs'] - 1:\n        save_path = os.path.join(output_dir, f\"checkpoint_epoch{epoch}\")\n        unwrapped_unet = unwrap_model(unet)\n        unet_lora_state_dict = convert_state_dict_to_diffusers(\n            get_peft_model_state_dict(unwrapped_unet)\n        )\n        StableDiffusionPipeline.save_lora_weights(\n            save_directory=save_path,\n            unet_lora_layers=unet_lora_state_dict,\n            is_main_process=True,\n            safe_serialization=True,\n        )\n        logger.info(f\"Saved state to {save_path}\")\n\n\n    if cfg.wandb:\n        wandb.finish()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-29T18:05:57.293609Z","iopub.execute_input":"2024-11-29T18:05:57.293965Z","iopub.status.idle":"2024-11-29T18:05:57.364181Z","shell.execute_reply.started":"2024-11-29T18:05:57.293925Z","shell.execute_reply":"2024-11-29T18:05:57.363309Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:05:57.365148Z","iopub.execute_input":"2024-11-29T18:05:57.365426Z","iopub.status.idle":"2024-11-29T18:10:09.167736Z","shell.execute_reply.started":"2024-11-29T18:05:57.365401Z","shell.execute_reply":"2024-11-29T18:10:09.144568Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275fcc9a3e66470fba9f7a8d9804d8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c732c44a977a454786f3ae47bdbb7f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce4947ddb174e9bac0c9309cdc43076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5903a871ed445ea23a38eea77b64c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282ceb811d084ccf8be726ab4001e563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b97b6769f94026a73f169105a8ef3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"903c382e12aa4ac28e396bae2c637ded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1efb345d8aa74e35ad548bfc2b90ade2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcff92c33341407abdaaf4679103c63e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c56aafabd7465593ee8f1056ab9bbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395aec9c133e4950bca86ab3db80802e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461f3caed12e402881836b8f8ad0e1c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc9096d182a4f329a29ee89c3c75a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d34357769d459ba062644e8ddf85b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801ef481b059453a925958928a4efcd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343f13db08814e9c951b5e49ff2adc33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"022dfb4d0feb454c86aced3d0e9950e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ImageReward.pt:   0%|          | 0.00/1.79G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b62dc44ea064827a69d916507e8e21c"}},"metadata":{}},{"name":"stdout","text":"load checkpoint from /root/.cache/ImageReward/ImageReward.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"med_config.json:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2452adb13134fc9b494bf9811b9c989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e602904adf45639ea9e76d55dd68b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a225326308b24affb2f1a667c4540be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a400e962509b40f0a90dfbd79caeb216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d88131290c42cf82e30b222d310cac"}},"metadata":{}},{"name":"stdout","text":"checkpoint loaded\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0: sampling:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Timestep:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3dc0b07caa84f67a90061a41316cf38"}},"metadata":{}},{"name":"stderr","text":"Epoch 0: sampling:  50%|█████     | 1/2 [01:38<01:38, 98.78s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Timestep:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083f015c1e6a4bf09a5e4551295f8231"}},"metadata":{}},{"name":"stderr","text":"Epoch 0: sampling: 100%|██████████| 2/2 [03:21<00:00, 100.59s/it]\nWaiting for rewards: 100%|██████████| 2/2 [00:00<00:00, 10727.12it/s]\nEpoch 0.0: training:   0%|          | 0/2 [00:00<?, ?it/s]\nTimestep:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\nEpoch 0.0: training:   0%|          | 0/2 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 397\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    394\u001b[0m     flow_next \u001b[38;5;241m=\u001b[39m flow_model(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_latents\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, j], timestep_next, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 397\u001b[0m     unet_output \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    398\u001b[0m     latent \u001b[38;5;241m=\u001b[39m pred_orig_latent(pipeline\u001b[38;5;241m.\u001b[39mscheduler, unet_output, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatents\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, j], sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, j])\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1216\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1214\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1216\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1226\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:1288\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m-> 1288\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:442\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    430\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    431\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    432\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention.py:507\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    505\u001b[0m gligen_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 507\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monly_cross_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    515\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention_processor.py:495\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2383\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2379\u001b[0m     key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mnorm_k(key)\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;66;03m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[0;32m-> 2383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2387\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n\u001b[1;32m   2388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 426.12 MiB is free. Process 2512 has 14.32 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 296.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 426.12 MiB is free. Process 2512 has 14.32 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 296.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}